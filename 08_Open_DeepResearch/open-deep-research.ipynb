{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research - Supervisor-Researcher Architecture\n",
    "\n",
    "In this notebook, we'll explore the **supervisor-researcher delegation architecture** for conducting deep research with LangGraph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "This implementation uses a **hierarchical delegation pattern** where:\n",
    "\n",
    "1. **User Clarification** - Optionally asks clarifying questions to understand the research scope\n",
    "2. **Research Brief Generation** - Transforms user messages into a structured research brief\n",
    "3. **Supervisor** - A lead researcher that analyzes the brief and delegates research tasks\n",
    "4. **Parallel Researchers** - Multiple sub-agents that conduct focused research simultaneously\n",
    "5. **Research Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings are combined into a comprehensive report\n",
    "\n",
    "![Architecture Diagram](https://i.imgur.com/Q8HEZn0.png)\n",
    "\n",
    "This differs from a section-based approach by allowing dynamic task decomposition based on the research question, rather than predefined sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ù Breakout Room #1\n",
    "## Deep Research Foundations\n",
    "\n",
    "In this breakout room, we'll understand the architecture and components of the Open Deep Research system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies\n",
    "\n",
    "You'll need API keys for Anthropic (for the LLM) and Tavily (for web search). We'll configure the system to use Anthropic's Claude Sonnet 4 exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: State Definitions\n",
    "\n",
    "The state structure is hierarchical with three levels:\n",
    "\n",
    "### Agent State (Top Level)\n",
    "Contains the overall conversation messages, research brief, accumulated notes, and final report.\n",
    "\n",
    "### Supervisor State (Middle Level)\n",
    "Manages the research supervisor's messages, research iterations, and coordinating parallel researchers.\n",
    "\n",
    "### Researcher State (Bottom Level)\n",
    "Each individual researcher has their own message history, tool call iterations, and research findings.\n",
    "\n",
    "We also have structured outputs for tool calling:\n",
    "- **ConductResearch** - Tool for supervisor to delegate research to a sub-agent\n",
    "- **ResearchComplete** - Tool to signal research phase is done\n",
    "- **ClarifyWithUser** - Structured output for asking clarifying questions\n",
    "- **ResearchQuestion** - Structured output for the research brief\n",
    "\n",
    "Let's import these from our library: [`open_deep_library/state.py`](open_deep_library/state.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import state definitions from the library\n",
    "from open_deep_library.state import (\n",
    "    # Main workflow states\n",
    "    AgentState,           # Lines 65-72: Top-level agent state with messages, research_brief, notes, final_report\n",
    "    AgentInputState,      # Lines 62-63: Input state is just messages\n",
    "    \n",
    "    # Supervisor states\n",
    "    SupervisorState,      # Lines 74-81: Supervisor manages research delegation and iterations\n",
    "    \n",
    "    # Researcher states\n",
    "    ResearcherState,      # Lines 83-90: Individual researcher with messages and tool iterations\n",
    "    ResearcherOutputState, # Lines 92-96: Output from researcher (compressed research + raw notes)\n",
    "    \n",
    "    # Structured outputs for tool calling\n",
    "    ConductResearch,      # Lines 15-19: Tool for delegating research to sub-agents\n",
    "    ResearchComplete,     # Lines 21-22: Tool to signal research completion\n",
    "    ClarifyWithUser,      # Lines 30-41: Structured output for user clarification\n",
    "    ResearchQuestion,     # Lines 43-48: Structured output for research brief\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Utility Functions and Tools\n",
    "\n",
    "The system uses several key utilities:\n",
    "\n",
    "### Search Tools\n",
    "- **tavily_search** - Async web search with automatic summarization to stay within token limits\n",
    "- Supports Anthropic native web search and Tavily API\n",
    "\n",
    "### Reflection Tools\n",
    "- **think_tool** - Allows researchers to reflect on their progress and plan next steps (ReAct pattern)\n",
    "\n",
    "### Helper Utilities\n",
    "- **get_all_tools** - Assembles the complete toolkit (search + MCP + reflection)\n",
    "- **get_today_str** - Provides current date context for research\n",
    "- Token limit handling utilities for graceful degradation\n",
    "\n",
    "These are defined in [`open_deep_library/utils.py`](open_deep_library/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-mcp-adapters in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-mcp-adapters) (1.2.9)\n",
      "Requirement already satisfied: mcp>=1.9.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-mcp-adapters) (1.26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.14.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-mcp-adapters) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.3.45)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (8.3.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (2.3.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (0.4.0)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (4.26.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain-mcp-adapters) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (0.0.20)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (3.2.0)\n",
      "Requirement already satisfied: starlette>=0.27 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (0.52.1)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.2->langchain-mcp-adapters) (0.34.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-mcp-adapters) (1.3.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters) (0.30.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp>=1.9.2->langchain-mcp-adapters) (1.0.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain-mcp-adapters) (44.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain-mcp-adapters) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain-mcp-adapters) (2.22)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from uvicorn>=0.31.1->mcp>=1.9.2->langchain-mcp-adapters) (8.1.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from click>=7.0->uvicorn>=0.31.1->mcp>=1.9.2->langchain-mcp-adapters) (0.4.6)\n",
      "Collecting open-deep-research\n",
      "  Downloading open_deep_research-0.0.16-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: langgraph>=0.2.55 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.0.7)\n",
      "Requirement already satisfied: langchain-community>=0.3.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.4.1)\n",
      "Requirement already satisfied: langchain-openai>=0.3.7 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.0.3)\n",
      "Requirement already satisfied: langchain-anthropic>=0.3.15 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.3.2)\n",
      "Requirement already satisfied: langchain-mcp-adapters>=0.1.6 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.2.1)\n",
      "Requirement already satisfied: langchain-deepseek>=0.1.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.1.3)\n",
      "Collecting langchain-tavily (from open-deep-research)\n",
      "  Downloading langchain_tavily-0.2.17-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting langchain-groq>=0.2.4 (from open-deep-research)\n",
      "  Downloading langchain_groq-1.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: openai>=1.61.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (2.15.0)\n",
      "Requirement already satisfied: tavily-python>=0.5.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.7.11)\n",
      "Collecting arxiv>=2.1.3 (from open-deep-research)\n",
      "  Downloading arxiv-2.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pymupdf>=1.25.3 (from open-deep-research)\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting xmltodict>=0.14.2 (from open-deep-research)\n",
      "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting linkup-sdk>=0.2.3 (from open-deep-research)\n",
      "  Downloading linkup_sdk-0.10.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: duckduckgo-search>=3.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (3.8.3)\n",
      "Collecting exa-py>=1.8.8 (from open-deep-research)\n",
      "  Downloading exa_py-2.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests>=2.32.3 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (2.32.5)\n",
      "Collecting beautifulsoup4==4.13.3 (from open-deep-research)\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.0.1)\n",
      "Collecting pytest (from open-deep-research)\n",
      "  Downloading pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: httpx>=0.24.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.28.1)\n",
      "Collecting markdownify>=0.11.6 (from open-deep-research)\n",
      "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: azure-identity>=1.21.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.24.0)\n",
      "Collecting azure-search>=1.0.0b2 (from open-deep-research)\n",
      "  Downloading azure_search-1.0.0b2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting azure-search-documents>=11.5.2 (from open-deep-research)\n",
      "  Downloading azure_search_documents-11.6.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: rich>=13.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (14.1.0)\n",
      "Collecting langgraph-cli>=0.3.1 (from langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading langgraph_cli-0.4.12-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: langsmith>=0.3.37 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.3.45)\n",
      "Collecting langchain-google-vertexai>=2.0.25 (from open-deep-research)\n",
      "  Downloading langchain_google_vertexai-3.2.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: langchain-google-genai>=2.1.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (4.2.0)\n",
      "Requirement already satisfied: ipykernel>=6.29.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (6.30.1)\n",
      "Collecting supabase>=2.15.3 (from open-deep-research)\n",
      "  Downloading supabase-2.27.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: mcp>=1.9.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (1.26.0)\n",
      "Collecting build>=1.2.2.post1 (from open-deep-research)\n",
      "  Using cached build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting setuptools>=80.9.0 (from open-deep-research)\n",
      "  Using cached setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel>=0.45.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from open-deep-research) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from beautifulsoup4==4.13.3->open-deep-research) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from beautifulsoup4==4.13.3->open-deep-research) (4.15.0)\n",
      "Collecting feedparser~=6.0.10 (from arxiv>=2.1.3->open-deep-research)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv>=2.1.3->open-deep-research)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests>=2.32.3->open-deep-research) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests>=2.32.3->open-deep-research) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests>=2.32.3->open-deep-research) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from requests>=2.32.3->open-deep-research) (2025.1.31)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-identity>=1.21.0->open-deep-research) (1.35.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-identity>=1.21.0->open-deep-research) (44.0.1)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-identity>=1.21.0->open-deep-research) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-identity>=1.21.0->open-deep-research) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-core>=1.31.0->azure-identity>=1.21.0->open-deep-research) (1.17.0)\n",
      "Collecting msrest>=0.6.10 (from azure-search>=1.0.0b2->open-deep-research)\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-common>=1.1 (from azure-search-documents>=11.5.2->open-deep-research)\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from azure-search-documents>=11.5.2->open-deep-research) (0.7.2)\n",
      "Requirement already satisfied: packaging>=24.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from build>=1.2.2.post1->open-deep-research) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.2.2.post1->open-deep-research)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from build>=1.2.2.post1->open-deep-research) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from cryptography>=2.5->azure-identity>=1.21.0->open-deep-research) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.21.0->open-deep-research) (2.22)\n",
      "Requirement already satisfied: aiofiles>=23.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from duckduckgo-search>=3.0.0->open-deep-research) (24.1.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from duckduckgo-search>=3.0.0->open-deep-research) (8.1.8)\n",
      "Requirement already satisfied: lxml>=4.9.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from duckduckgo-search>=3.0.0->open-deep-research) (5.3.1)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from exa-py>=1.8.8->open-deep-research) (1.0.9)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from exa-py>=1.8.8->open-deep-research) (2.12.5)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpcore>=1.0.9->exa-py>=1.8.8->open-deep-research) (0.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx>=0.24.0->open-deep-research) (4.8.0)\n",
      "Requirement already satisfied: brotli in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search>=3.0.0->open-deep-research) (1.1.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search>=3.0.0->open-deep-research) (4.2.0)\n",
      "Requirement already satisfied: socksio==1.* in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search>=3.0.0->open-deep-research) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search>=3.0.0->open-deep-research) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.24.1->duckduckgo-search>=3.0.0->open-deep-research) (4.1.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (9.5.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (6.5.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipykernel>=6.29.5->open-deep-research) (5.14.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel>=6.29.5->open-deep-research) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5->open-deep-research) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5->open-deep-research) (311)\n",
      "Requirement already satisfied: anthropic<1.0.0,>=0.78.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-anthropic>=0.3.15->open-deep-research) (0.79.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-anthropic>=0.3.15->open-deep-research) (1.2.9)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic>=0.3.15->open-deep-research) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic>=0.3.15->open-deep-research) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic>=0.3.15->open-deep-research) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from anthropic<1.0.0,>=0.78.0->langchain-anthropic>=0.3.15->open-deep-research) (1.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.9->langchain-anthropic>=0.3.15->open-deep-research) (1.33)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.9->langchain-anthropic>=0.3.15->open-deep-research) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.9->langchain-anthropic>=0.3.15->open-deep-research) (8.3.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.9->langchain-anthropic>=0.3.15->open-deep-research) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.9->langchain-anthropic>=0.3.15->open-deep-research) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith>=0.3.37->open-deep-research) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith>=0.3.37->open-deep-research) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langsmith>=0.3.37->open-deep-research) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic>=2.10.6->exa-py>=1.8.8->open-deep-research) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic>=2.10.6->exa-py>=1.8.8->open-deep-research) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pydantic>=2.10.6->exa-py>=1.8.8->open-deep-research) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-community>=0.3.9->open-deep-research) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.9->open-deep-research) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.9->open-deep-research) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community>=0.3.9->open-deep-research) (1.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community>=0.3.9->open-deep-research) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.3.9->open-deep-research) (1.0.0)\n",
      "INFO: pip is looking at multiple versions of langchain-deepseek to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-deepseek>=0.1.2 (from open-deep-research)\n",
      "  Downloading langchain_deepseek-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-openai>=0.3.7->open-deep-research) (0.7.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from openai>=1.61.0->open-deep-research) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.3.7->open-deep-research) (2024.11.6)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-google-genai>=2.1.5->open-deep-research) (1.2.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-google-genai>=2.1.5->open-deep-research) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai>=2.1.5->open-deep-research) (2.48.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai>=2.1.5->open-deep-research) (15.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai>=2.1.5->open-deep-research) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai>=2.1.5->open-deep-research) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai>=2.1.5->open-deep-research) (0.6.2)\n",
      "Collecting bottleneck<2.0.0,>=1.4.0 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading bottleneck-1.6.0-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.97.0 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_aiplatform-1.136.0-py2.py3-none-any.whl.metadata (46 kB)\n",
      "Collecting google-cloud-storage<4.0.0,>=2.18.0 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_storage-3.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting google-cloud-vectorsearch>=0.2.0 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_vectorsearch-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain-tests>=1.0.1 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading langchain_tests-1.1.4-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.6 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Using cached numexpr-2.14.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyarrow<23.0.0,>=19.0.1 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting validators<1.0.0,>=0.22.0 (from langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_api_core-2.29.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading proto_plus-1.27.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research) (6.33.4)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_bigquery-3.40.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_resource_manager-1.16.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research) (1.76.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading grpcio_status-1.78.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage<4.0.0,>=2.18.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading google_crc32c-1.8.0-cp312-cp312-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.97.0->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading grpcio-1.78.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq>=0.2.4->open-deep-research)\n",
      "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pytest-asyncio<2.0.0,>=0.20.0 (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pytest_asyncio-1.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pytest-benchmark (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pytest_benchmark-5.2.3-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pytest-codspeed (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pytest_codspeed-4.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pytest-recording (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pytest-socket<1.0.0,>=0.7.0 (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pytest (from open-deep-research)\n",
      "  Downloading pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting syrupy<5.0.0,>=4.0.0 (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting vcrpy<9.0.0,>=8.0.0 (from langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading vcrpy-8.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting iniconfig>=1 (from pytest->open-deep-research)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->open-deep-research)\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from vcrpy<9.0.0,>=8.0.0->langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research) (1.17.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph>=0.2.55->open-deep-research) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph>=0.2.55->open-deep-research) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph>=0.2.55->open-deep-research) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph>=0.2.55->open-deep-research) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph>=0.2.55->open-deep-research) (1.12.2)\n",
      "Collecting langgraph-api<0.8.0,>=0.5.35 (from langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading langgraph_api-0.7.28-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langgraph-runtime-inmem>=0.7 (from langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading langgraph_runtime_inmem-0.24.0-py3-none-any.whl.metadata (601 bytes)\n",
      "Collecting cloudpickle>=3.0.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting grpcio-health-checking<1.77.0,>=1.76.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading grpcio_health_checking-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting grpcio-tools==1.76.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading grpcio_tools-1.76.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "INFO: pip is looking at multiple versions of langgraph-api to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langgraph-api<0.8.0,>=0.5.35 (from langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading langgraph_api-0.7.27-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.75.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading grpcio_health_checking-1.78.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting grpcio-tools==1.75.1 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading grpcio_tools-1.75.1-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting jsonschema-rs<0.30,>=0.20.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading jsonschema_rs-0.29.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting langsmith>=0.3.37 (from open-deep-research)\n",
      "  Downloading langsmith-0.7.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting opentelemetry-api>=1.37.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Using cached opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http>=1.37.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.37.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyjwt>=2.9.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (2.10.1)\n",
      "Collecting sse-starlette<2.2.0,>=2.1.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: starlette>=0.38.6 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (0.52.1)\n",
      "Collecting structlog<26,>=24.1.0 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading structlog-25.5.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting truststore>=0.1 (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading truststore-0.10.4-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.26.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (0.34.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (1.1.0)\n",
      "Collecting blockbuster<2.0.0,>=1.5.24 (from langgraph-runtime-inmem>=0.7->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading blockbuster-1.5.26-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting croniter>=1.0.1 (from langgraph-runtime-inmem>=0.7->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading croniter-6.0.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting forbiddenfruit>=0.1.4 (from blockbuster<2.0.0,>=1.5.24->langgraph-runtime-inmem>=0.7->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading forbiddenfruit-0.1.4.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz>2021.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from croniter>=1.0.1->langgraph-runtime-inmem>=0.7->langgraph-cli[inmem]>=0.3.1->open-deep-research) (2025.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.4->open-deep-research) (4.26.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from mcp>=1.9.4->open-deep-research) (0.0.20)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->open-deep-research) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->open-deep-research) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from jsonschema>=4.20.0->mcp>=1.9.4->open-deep-research) (0.30.0)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.6.10->azure-search>=1.0.0b2->open-deep-research)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from opentelemetry-api>=1.37.0->langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.37.0->langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research) (3.23.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http>=1.37.0->langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-http>=1.37.0->langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.37.0->langgraph-api<0.8.0,>=0.5.35->langgraph-cli[inmem]>=0.3.1->open-deep-research)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-search>=1.0.0b2->open-deep-research)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from rich>=13.0.0->open-deep-research) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->open-deep-research) (0.1.2)\n",
      "Collecting realtime==2.27.3 (from supabase>=2.15.3->open-deep-research)\n",
      "  Downloading realtime-2.27.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting supabase-functions==2.27.3 (from supabase>=2.15.3->open-deep-research)\n",
      "  Downloading supabase_functions-2.27.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting storage3==2.27.3 (from supabase>=2.15.3->open-deep-research)\n",
      "  Downloading storage3-2.27.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting supabase-auth==2.27.3 (from supabase>=2.15.3->open-deep-research)\n",
      "  Downloading supabase_auth-2.27.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting postgrest==2.27.3 (from supabase>=2.15.3->open-deep-research)\n",
      "  Downloading postgrest-2.27.3-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Collecting deprecation>=2.1.0 (from postgrest==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyiceberg>=0.10.0 (from storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading pyiceberg-0.11.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting strenum>=0.4.15 (from supabase-functions==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting mmh3<6.0.0,>=4.0.0 (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading mmh3-5.2.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting strictyaml<2.0.0,>=1.7.0 (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading strictyaml-1.7.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research) (2025.2.0)\n",
      "Collecting pyparsing<4.0.0,>=3.1.0 (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyroaring<2.0.0,>=1.0.0 (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading pyroaring-1.0.3-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting cachetools<7.0,>=5.5 (from pyiceberg>=0.10.0->storage3==2.27.3->supabase>=2.15.3->open-deep-research)\n",
      "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community>=0.3.9->open-deep-research)\n",
      "  Using cached aiohttp-3.13.3-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.20 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from langchain-tavily->open-deep-research) (1.2.9)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.9->open-deep-research)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting py-cpuinfo (from pytest-benchmark->langchain-tests>=1.0.1->langchain-google-vertexai>=2.0.25->open-deep-research)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\sinhak\\appdata\\local\\miniconda3\\envs\\agentenv312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->open-deep-research) (0.2.3)\n",
      "Downloading open_deep_research-0.0.16-py3-none-any.whl (70 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading arxiv-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Downloading azure_search-1.0.0b2-py2.py3-none-any.whl (46 kB)\n",
      "Downloading azure_search_documents-11.6.0-py3-none-any.whl (307 kB)\n",
      "Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Using cached build-1.4.0-py3-none-any.whl (24 kB)\n",
      "Downloading exa_py-2.4.0-py3-none-any.whl (63 kB)\n",
      "Downloading langchain_deepseek-1.0.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading langchain_google_vertexai-3.2.2-py3-none-any.whl (113 kB)\n",
      "Downloading bottleneck-1.6.0-cp312-cp312-win_amd64.whl (113 kB)\n",
      "Downloading google_cloud_aiplatform-1.136.0-py2.py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/8.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/8.2 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.2 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.5/8.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.2/8.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 5.0 MB/s  0:00:01\n",
      "Downloading google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "Downloading google_cloud_bigquery-3.40.0-py3-none-any.whl (261 kB)\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_resource_manager-1.16.0-py3-none-any.whl (400 kB)\n",
      "Downloading google_cloud_storage-3.9.0-py3-none-any.whl (321 kB)\n",
      "Downloading google_crc32c-1.8.0-cp312-cp312-win_amd64.whl (34 kB)\n",
      "Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl (32 kB)\n",
      "Downloading grpcio_status-1.78.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.78.0-cp312-cp312-win_amd64.whl (4.8 MB)\n",
      "   ---------------------------------------- 0.0/4.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.8/4.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.1/4.8 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.1/4.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.2/4.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.8/4.8 MB 5.4 MB/s  0:00:00\n",
      "Downloading numexpr-2.14.1-cp312-cp312-win_amd64.whl (160 kB)\n",
      "Downloading proto_plus-1.27.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 4.2 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/28.0 MB 5.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.9/28.0 MB 5.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 4.2/28.0 MB 5.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.8/28.0 MB 6.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 7.1/28.0 MB 6.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 8.4/28.0 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 9.4/28.0 MB 6.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 10.7/28.0 MB 6.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 12.1/28.0 MB 6.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 13.4/28.0 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 14.7/28.0 MB 6.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 17.3/28.0 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.4/28.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 19.7/28.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 21.0/28.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.0/28.0 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.3/28.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.4/28.0 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 5.9 MB/s  0:00:04\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading google_cloud_vectorsearch-0.4.0-py3-none-any.whl (177 kB)\n",
      "Downloading langchain_groq-1.1.2-py3-none-any.whl (19 kB)\n",
      "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
      "Downloading langchain_tests-1.1.4-py3-none-any.whl (55 kB)\n",
      "Downloading pytest-8.4.2-py3-none-any.whl (365 kB)\n",
      "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading pytest_asyncio-1.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
      "Downloading vcrpy-8.1.1-py3-none-any.whl (42 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading langgraph_cli-0.4.12-py3-none-any.whl (41 kB)\n",
      "Downloading langgraph_api-0.7.27-py3-none-any.whl (486 kB)\n",
      "Downloading grpcio_tools-1.75.1-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.0/1.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 5.2 MB/s  0:00:00\n",
      "Downloading grpcio_health_checking-1.78.0-py3-none-any.whl (19 kB)\n",
      "Downloading jsonschema_rs-0.29.1-cp312-cp312-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 4.9 MB/s  0:00:00\n",
      "Downloading langgraph_runtime_inmem-0.24.0-py3-none-any.whl (41 kB)\n",
      "Downloading blockbuster-1.5.26-py3-none-any.whl (13 kB)\n",
      "Downloading langsmith-0.7.1-py3-none-any.whl (322 kB)\n",
      "Downloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "Downloading structlog-25.5.0-py3-none-any.whl (72 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading croniter-6.0.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading linkup_sdk-0.10.0-py3-none-any.whl (11 kB)\n",
      "Downloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
      "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Using cached opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading pymupdf-1.26.7-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/18.4 MB 4.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.4/18.4 MB 5.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.4/18.4 MB 5.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 4.5/18.4 MB 5.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.8/18.4 MB 2.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 5.8/18.4 MB 2.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 6.0/18.4 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.0/18.4 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 6.6/18.4 MB 2.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.3/18.4 MB 2.0 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.4/18.4 MB 2.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.0/18.4 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 11.8/18.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 14.4/18.4 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.0/18.4 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.3/18.4 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 3.2 MB/s  0:00:05\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached setuptools-82.0.0-py3-none-any.whl (1.0 MB)\n",
      "Downloading supabase-2.27.3-py3-none-any.whl (16 kB)\n",
      "Downloading postgrest-2.27.3-py3-none-any.whl (22 kB)\n",
      "Downloading realtime-2.27.3-py3-none-any.whl (22 kB)\n",
      "Downloading storage3-2.27.3-py3-none-any.whl (27 kB)\n",
      "Downloading supabase_auth-2.27.3-py3-none-any.whl (48 kB)\n",
      "Downloading supabase_functions-2.27.3-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pyiceberg-0.11.0-cp312-cp312-win_amd64.whl (531 kB)\n",
      "   ---------------------------------------- 0.0/531.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/531.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 531.0/531.0 kB 5.7 MB/s  0:00:00\n",
      "Downloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
      "Downloading mmh3-5.2.0-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Downloading pyroaring-1.0.3-cp312-cp312-win_amd64.whl (260 kB)\n",
      "Downloading strictyaml-1.7.3-py3-none-any.whl (123 kB)\n",
      "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading truststore-0.10.4-py3-none-any.whl (18 kB)\n",
      "Downloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading langchain_tavily-0.2.17-py3-none-any.whl (30 kB)\n",
      "Using cached aiohttp-3.13.3-cp312-cp312-win_amd64.whl (455 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading pytest_benchmark-5.2.3-py3-none-any.whl (45 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pytest_codspeed-4.3.0-py3-none-any.whl (125 kB)\n",
      "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: forbiddenfruit, sgmllib3k\n",
      "  Building wheel for forbiddenfruit (setup.py): started\n",
      "  Building wheel for forbiddenfruit (setup.py): finished with status 'done'\n",
      "  Created wheel for forbiddenfruit: filename=forbiddenfruit-0.1.4-py3-none-any.whl size=21929 sha256=1cbce8ab8403586ce188de24c0ef5e71f04128a8bf2347986daac7ae74c9b5c5\n",
      "  Stored in directory: c:\\users\\sinhak\\appdata\\local\\pip\\cache\\wheels\\eb\\1b\\4e\\1720775f695118457d0692cea72b8be2b8af2a7bae46611e93\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=f9d0e5995741a1adfdf2454c3d8b9cc9f313c33e0b9ca0f9670ca5cdf7aa319f\n",
      "  Stored in directory: c:\\users\\sinhak\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built forbiddenfruit sgmllib3k\n",
      "Installing collected packages: strenum, sgmllib3k, pyroaring, py-cpuinfo, forbiddenfruit, azure-common, yarl, xmltodict, vcrpy, validators, truststore, structlog, setuptools, pyproject_hooks, pyparsing, pymupdf, pyarrow, proto-plus, pluggy, opentelemetry-proto, oauthlib, numexpr, mmh3, jsonschema-rs, iniconfig, grpcio, googleapis-common-protos, google-crc32c, feedparser, deprecation, cloudpickle, cachetools, bottleneck, blockbuster, beautifulsoup4, aiosignal, aiohappyeyeballs, strictyaml, requests-oauthlib, pytest, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, markdownify, grpcio-tools, grpcio-status, grpcio-health-checking, google-resumable-media, croniter, build, arxiv, aiohttp, syrupy, sse-starlette, realtime, pytest-socket, pytest-recording, pytest-codspeed, pytest-benchmark, pytest-asyncio, pyiceberg, opentelemetry-semantic-conventions, msrest, linkup-sdk, langsmith, grpc-google-iam-v1, groq, azure-search-documents, supabase-functions, supabase-auth, storage3, postgrest, opentelemetry-sdk, langgraph-cli, google-api-core, exa-py, azure-search, supabase, opentelemetry-exporter-otlp-proto-http, langchain-tests, langchain-groq, google-cloud-core, langchain-deepseek, google-cloud-vectorsearch, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform, langgraph-runtime-inmem, langchain-google-vertexai, langgraph-api, langchain-tavily, open-deep-research\n",
      "\n",
      "   - --------------------------------------  3/92 [py-cpuinfo]\n",
      "   - --------------------------------------  3/92 [py-cpuinfo]\n",
      "   -- -------------------------------------  5/92 [azure-common]\n",
      "  Attempting uninstall: yarl\n",
      "   -- -------------------------------------  5/92 [azure-common]\n",
      "    Found existing installation: yarl 1.18.3\n",
      "   -- -------------------------------------  5/92 [azure-common]\n",
      "    Uninstalling yarl-1.18.3:\n",
      "   -- -------------------------------------  5/92 [azure-common]\n",
      "      Successfully uninstalled yarl-1.18.3\n",
      "   -- -------------------------------------  5/92 [azure-common]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   -- -------------------------------------  6/92 [yarl]\n",
      "   --- ------------------------------------  8/92 [vcrpy]\n",
      "   --- ------------------------------------  8/92 [vcrpy]\n",
      "   --- ------------------------------------  9/92 [validators]\n",
      "   ---- ----------------------------------- 10/92 [truststore]\n",
      "   ---- ----------------------------------- 11/92 [structlog]\n",
      "  Attempting uninstall: setuptools\n",
      "   ---- ----------------------------------- 11/92 [structlog]\n",
      "    Found existing installation: setuptools 78.1.1\n",
      "   ---- ----------------------------------- 11/92 [structlog]\n",
      "    Uninstalling setuptools-78.1.1:\n",
      "   ---- ----------------------------------- 11/92 [structlog]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "      Successfully uninstalled setuptools-78.1.1\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ----- ---------------------------------- 12/92 [setuptools]\n",
      "   ------ --------------------------------- 14/92 [pyparsing]\n",
      "   ------ --------------------------------- 15/92 [pymupdf]\n",
      "   ------ --------------------------------- 15/92 [pymupdf]\n",
      "   ------ --------------------------------- 15/92 [pymupdf]\n",
      "   ------ --------------------------------- 15/92 [pymupdf]\n",
      "   ------ --------------------------------- 15/92 [pymupdf]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------ --------------------------------- 16/92 [pyarrow]\n",
      "   ------- -------------------------------- 17/92 [proto-plus]\n",
      "   ------- -------------------------------- 18/92 [pluggy]\n",
      "   -------- ------------------------------- 19/92 [opentelemetry-proto]\n",
      "   -------- ------------------------------- 20/92 [oauthlib]\n",
      "   -------- ------------------------------- 20/92 [oauthlib]\n",
      "   -------- ------------------------------- 20/92 [oauthlib]\n",
      "   -------- ------------------------------- 20/92 [oauthlib]\n",
      "   --------- ------------------------------ 22/92 [mmh3]\n",
      "  Attempting uninstall: grpcio\n",
      "   --------- ------------------------------ 22/92 [mmh3]\n",
      "    Found existing installation: grpcio 1.76.0\n",
      "   --------- ------------------------------ 22/92 [mmh3]\n",
      "    Uninstalling grpcio-1.76.0:\n",
      "   --------- ------------------------------ 22/92 [mmh3]\n",
      "      Successfully uninstalled grpcio-1.76.0\n",
      "   --------- ------------------------------ 22/92 [mmh3]\n",
      "   ---------- ----------------------------- 25/92 [grpcio]\n",
      "   ---------- ----------------------------- 25/92 [grpcio]\n",
      "   ---------- ----------------------------- 25/92 [grpcio]\n",
      "   ---------- ----------------------------- 25/92 [grpcio]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ----------- ---------------------------- 26/92 [googleapis-common-protos]\n",
      "   ------------ --------------------------- 28/92 [feedparser]\n",
      "   ------------ --------------------------- 28/92 [feedparser]\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "   ------------- -------------------------- 32/92 [bottleneck]\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "  Attempting uninstall: aiosignal\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "    Found existing installation: aiosignal 1.3.2\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "    Uninstalling aiosignal-1.3.2:\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "      Successfully uninstalled aiosignal-1.3.2\n",
      "   -------------- ------------------------- 34/92 [beautifulsoup4]\n",
      "   --------------- ------------------------ 35/92 [aiosignal]\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "   --------------- ------------------------ 35/92 [aiosignal]\n",
      "    Found existing installation: aiohappyeyeballs 2.4.6\n",
      "   --------------- ------------------------ 35/92 [aiosignal]\n",
      "    Uninstalling aiohappyeyeballs-2.4.6:\n",
      "   --------------- ------------------------ 35/92 [aiosignal]\n",
      "      Successfully uninstalled aiohappyeyeballs-2.4.6\n",
      "   --------------- ------------------------ 35/92 [aiosignal]\n",
      "   ---------------- ----------------------- 37/92 [strictyaml]\n",
      "   ---------------- ----------------------- 37/92 [strictyaml]\n",
      "   ---------------- ----------------------- 37/92 [strictyaml]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "    Found existing installation: opentelemetry-api 1.36.0\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "    Uninstalling opentelemetry-api-1.36.0:\n",
      "   ---------------- ----------------------- 39/92 [pytest]\n",
      "   ----------------- ---------------------- 41/92 [opentelemetry-api]\n",
      "      Successfully uninstalled opentelemetry-api-1.36.0\n",
      "   ----------------- ---------------------- 41/92 [opentelemetry-api]\n",
      "   ----------------- ---------------------- 41/92 [opentelemetry-api]\n",
      "   ----------------- ---------------------- 41/92 [opentelemetry-api]\n",
      "   ------------------ --------------------- 42/92 [markdownify]\n",
      "   ------------------ --------------------- 43/92 [grpcio-tools]\n",
      "   ------------------ --------------------- 43/92 [grpcio-tools]\n",
      "   ------------------- -------------------- 45/92 [grpcio-health-checking]\n",
      "   -------------------- ------------------- 46/92 [google-resumable-media]\n",
      "   -------------------- ------------------- 48/92 [build]\n",
      "   -------------------- ------------------- 48/92 [build]\n",
      "   -------------------- ------------------- 48/92 [build]\n",
      "   -------------------- ------------------- 48/92 [build]\n",
      "   --------------------- ------------------ 49/92 [arxiv]\n",
      "  Attempting uninstall: aiohttp\n",
      "   --------------------- ------------------ 49/92 [arxiv]\n",
      "    Found existing installation: aiohttp 3.11.12\n",
      "   --------------------- ------------------ 49/92 [arxiv]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "    Uninstalling aiohttp-3.11.12:\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "      Successfully uninstalled aiohttp-3.11.12\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   --------------------- ------------------ 50/92 [aiohttp]\n",
      "   ---------------------- ----------------- 51/92 [syrupy]\n",
      "  Attempting uninstall: sse-starlette\n",
      "   ---------------------- ----------------- 51/92 [syrupy]\n",
      "    Found existing installation: sse-starlette 3.2.0\n",
      "   ---------------------- ----------------- 51/92 [syrupy]\n",
      "    Uninstalling sse-starlette-3.2.0:\n",
      "   ---------------------- ----------------- 51/92 [syrupy]\n",
      "      Successfully uninstalled sse-starlette-3.2.0\n",
      "   ---------------------- ----------------- 51/92 [syrupy]\n",
      "   ---------------------- ----------------- 52/92 [sse-starlette]\n",
      "   ----------------------- ---------------- 53/92 [realtime]\n",
      "   ----------------------- ---------------- 55/92 [pytest-recording]\n",
      "   ------------------------ --------------- 56/92 [pytest-codspeed]\n",
      "   ------------------------ --------------- 57/92 [pytest-benchmark]\n",
      "   ------------------------ --------------- 57/92 [pytest-benchmark]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   ------------------------- -------------- 59/92 [pyiceberg]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   --------------------- ----------- 60/92 [opentelemetry-semantic-conventions]\n",
      "   -------------------------- ------------- 61/92 [msrest]\n",
      "   -------------------------- ------------- 62/92 [linkup-sdk]\n",
      "  Attempting uninstall: langsmith\n",
      "   -------------------------- ------------- 62/92 [linkup-sdk]\n",
      "    Found existing installation: langsmith 0.3.45\n",
      "   -------------------------- ------------- 62/92 [linkup-sdk]\n",
      "    Uninstalling langsmith-0.3.45:\n",
      "   -------------------------- ------------- 62/92 [linkup-sdk]\n",
      "      Successfully uninstalled langsmith-0.3.45\n",
      "   -------------------------- ------------- 62/92 [linkup-sdk]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 63/92 [langsmith]\n",
      "   --------------------------- ------------ 64/92 [grpc-google-iam-v1]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 65/92 [groq]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ---------------------------- ----------- 66/92 [azure-search-documents]\n",
      "   ----------------------------- ---------- 67/92 [supabase-functions]\n",
      "   ----------------------------- ---------- 68/92 [supabase-auth]\n",
      "   ----------------------------- ---------- 68/92 [supabase-auth]\n",
      "   ------------------------------ --------- 69/92 [storage3]\n",
      "   ------------------------------ --------- 70/92 [postgrest]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------ --------- 71/92 [opentelemetry-sdk]\n",
      "   ------------------------------- -------- 72/92 [langgraph-cli]\n",
      "   ------------------------------- -------- 72/92 [langgraph-cli]\n",
      "   ------------------------------- -------- 72/92 [langgraph-cli]\n",
      "   ------------------------------- -------- 73/92 [google-api-core]\n",
      "   ------------------------------- -------- 73/92 [google-api-core]\n",
      "   ------------------------------- -------- 73/92 [google-api-core]\n",
      "   ------------------------------- -------- 73/92 [google-api-core]\n",
      "   ------------------------------- -------- 73/92 [google-api-core]\n",
      "   -------------------------------- ------- 74/92 [exa-py]\n",
      "   -------------------------------- ------- 74/92 [exa-py]\n",
      "   -------------------------------- ------- 74/92 [exa-py]\n",
      "   -------------------------------- ------- 75/92 [azure-search]\n",
      "   -------------------------------- ------- 75/92 [azure-search]\n",
      "   --------------------------------- ------ 76/92 [supabase]\n",
      "   ------------------------ ---- 77/92 [opentelemetry-exporter-otlp-proto-http]\n",
      "   --------------------------------- ------ 78/92 [langchain-tests]\n",
      "   --------------------------------- ------ 78/92 [langchain-tests]\n",
      "   ---------------------------------- ----- 80/92 [google-cloud-core]\n",
      "  Attempting uninstall: langchain-deepseek\n",
      "   ---------------------------------- ----- 80/92 [google-cloud-core]\n",
      "    Found existing installation: langchain-deepseek 0.1.3\n",
      "   ---------------------------------- ----- 80/92 [google-cloud-core]\n",
      "    Uninstalling langchain-deepseek-0.1.3:\n",
      "   ---------------------------------- ----- 80/92 [google-cloud-core]\n",
      "   ----------------------------------- ---- 81/92 [langchain-deepseek]\n",
      "      Successfully uninstalled langchain-deepseek-0.1.3\n",
      "   ----------------------------------- ---- 81/92 [langchain-deepseek]\n",
      "   ----------------------------------- ---- 82/92 [google-cloud-vectorsearch]\n",
      "   ----------------------------------- ---- 82/92 [google-cloud-vectorsearch]\n",
      "   ----------------------------------- ---- 82/92 [google-cloud-vectorsearch]\n",
      "   ----------------------------------- ---- 82/92 [google-cloud-vectorsearch]\n",
      "   ------------------------------------ --- 83/92 [google-cloud-storage]\n",
      "   ------------------------------------ --- 83/92 [google-cloud-storage]\n",
      "   ------------------------------------ --- 83/92 [google-cloud-storage]\n",
      "   ------------------------------------ --- 83/92 [google-cloud-storage]\n",
      "   ------------------------------------ --- 83/92 [google-cloud-storage]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ---------------------------------- --- 84/92 [google-cloud-resource-manager]\n",
      "   ------------------------------------ --- 85/92 [google-cloud-bigquery]\n",
      "   ------------------------------------ --- 85/92 [google-cloud-bigquery]\n",
      "   ------------------------------------ --- 85/92 [google-cloud-bigquery]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 86/92 [google-cloud-aiplatform]\n",
      "   -------------------------------------- - 88/92 [langchain-google-vertexai]\n",
      "   -------------------------------------- - 88/92 [langchain-google-vertexai]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   -------------------------------------- - 89/92 [langgraph-api]\n",
      "   ---------------------------------------  91/92 [open-deep-research]\n",
      "   ---------------------------------------- 92/92 [open-deep-research]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 arxiv-2.4.0 azure-common-1.1.28 azure-search-1.0.0b2 azure-search-documents-11.6.0 beautifulsoup4-4.13.3 blockbuster-1.5.26 bottleneck-1.6.0 build-1.4.0 cachetools-6.2.6 cloudpickle-3.1.2 croniter-6.0.0 deprecation-2.1.0 exa-py-2.4.0 feedparser-6.0.12 forbiddenfruit-0.1.4 google-api-core-2.29.0 google-cloud-aiplatform-1.136.0 google-cloud-bigquery-3.40.0 google-cloud-core-2.5.0 google-cloud-resource-manager-1.16.0 google-cloud-storage-3.9.0 google-cloud-vectorsearch-0.4.0 google-crc32c-1.8.0 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 groq-0.37.1 grpc-google-iam-v1-0.14.3 grpcio-1.78.0 grpcio-health-checking-1.78.0 grpcio-status-1.78.0 grpcio-tools-1.75.1 iniconfig-2.3.0 jsonschema-rs-0.29.1 langchain-deepseek-1.0.1 langchain-google-vertexai-3.2.2 langchain-groq-1.1.2 langchain-tavily-0.2.17 langchain-tests-1.1.4 langgraph-api-0.7.27 langgraph-cli-0.4.12 langgraph-runtime-inmem-0.24.0 langsmith-0.7.1 linkup-sdk-0.10.0 markdownify-1.2.2 mmh3-5.2.0 msrest-0.7.1 numexpr-2.14.1 oauthlib-3.3.1 open-deep-research-0.0.16 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-http-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 pluggy-1.6.0 postgrest-2.27.3 proto-plus-1.27.1 py-cpuinfo-9.0.0 pyarrow-22.0.0 pyiceberg-0.11.0 pymupdf-1.26.7 pyparsing-3.3.2 pyproject_hooks-1.2.0 pyroaring-1.0.3 pytest-8.4.2 pytest-asyncio-1.3.0 pytest-benchmark-5.2.3 pytest-codspeed-4.3.0 pytest-recording-0.13.4 pytest-socket-0.7.0 realtime-2.27.3 requests-oauthlib-2.0.0 setuptools-82.0.0 sgmllib3k-1.0.0 sse-starlette-2.1.3 storage3-2.27.3 strenum-0.4.15 strictyaml-1.7.3 structlog-25.5.0 supabase-2.27.3 supabase-auth-2.27.3 supabase-functions-2.27.3 syrupy-4.9.1 truststore-0.10.4 validators-0.35.0 vcrpy-8.1.1 xmltodict-1.0.2 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'forbiddenfruit' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'forbiddenfruit'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SinhaK\\AppData\\Local\\miniconda3\\envs\\agentenv312\\Lib\\site-packages\\~arl'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SinhaK\\AppData\\Local\\miniconda3\\envs\\agentenv312\\Lib\\site-packages\\~iohttp'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gemini-lib 2.1.7rc8 requires anthropic==0.34.1, but you have anthropic 0.79.0 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires beautifulsoup4==4.12.2, but you have beautifulsoup4 4.13.3 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires httpx==0.27.0, but you have httpx 0.28.1 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain==0.3.20, but you have langchain 1.2.9 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain-anthropic==0.2.3, but you have langchain-anthropic 1.3.2 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain_community==0.3.19, but you have langchain-community 0.4.1 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain_core==0.3.74, but you have langchain-core 1.2.9 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain-deepseek==0.1.3, but you have langchain-deepseek 1.0.1 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langchain_openai==0.3.16, but you have langchain-openai 1.0.3 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires langgraph==0.2.36, but you have langgraph 1.0.7 which is incompatible.\n",
      "gemini-lib 2.1.7rc8 requires requests==2.32.2, but you have requests 2.32.5 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires langchain-core<0.4.0,>=0.3.0, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-azure-ai 0.1.2 requires langchain-openai<0.4.0,>=0.3.0, but you have langchain-openai 1.0.3 which is incompatible.\n",
      "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.4.1 which is incompatible.\n",
      "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 1.2.9 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-mcp-adapters\n",
    "!pip install open-deep-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "start = Path(r\"c:\\MyWorkspace\\Assignments\\AIE9\") # adjust if needed\n",
    "matches = []\n",
    "\n",
    "for p in start.rglob(\"open_deep_research\"):\n",
    "    if p.is_dir():\n",
    "        matches.append(p)\n",
    "\n",
    "print(\"Found:\", len(matches))\n",
    "for m in matches[:20]:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0] = c:\\MyWorkspace\\Assignments\\AIE9\\08_Open_DeepResearch\n",
      "‚úÖ imports ok\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "SRC = ROOT / \"src\"\n",
    "if SRC.exists():\n",
    "    sys.path.insert(0, str(SRC))\n",
    "else:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"sys.path[0] =\", sys.path[0])\n",
    "\n",
    "from open_deep_library.utils import tavily_search, think_tool\n",
    "print(\"‚úÖ imports ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Safe imports loaded (open_deep_research not required).\n",
      "‚úÖ get_all_tools imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPEN DEEP RESEARCH ‚Äî SAFE IMPORTS (works even if open_deep_research is missing)\n",
    "# Paste + run this cell.\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure current folder is importable (contains open_deep_library/)\n",
    "ROOT = Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Import ONLY symbols that do NOT require open_deep_research to exist.\n",
    "# (Your env confirms tavily_search / think_tool import successfully.)\n",
    "from open_deep_library.utils import (\n",
    "    tavily_search,\n",
    "    think_tool,\n",
    "    get_today_str,\n",
    "    get_api_key_for_model,\n",
    "    is_token_limit_exceeded,\n",
    "    get_model_token_limit,\n",
    "    remove_up_to_last_ai_message,\n",
    "    anthropic_websearch_called,\n",
    "    openai_websearch_called,\n",
    "    get_notes_from_tool_calls,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Safe imports loaded (open_deep_research not required).\")\n",
    "\n",
    "# Optional: try importing get_all_tools (will fail if it depends on open_deep_research)\n",
    "try:\n",
    "    from open_deep_library.utils import get_all_tools\n",
    "    print(\"‚úÖ get_all_tools imported successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è get_all_tools NOT available in this environment.\")\n",
    "    print(\" Reason:\", repr(e))\n",
    "    print(\" Fix: restore/install the missing 'open_deep_research' package OR patch utils.py to make it optional.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Configuration System\n",
    "\n",
    "The configuration system controls:\n",
    "\n",
    "### Research Behavior\n",
    "- **allow_clarification** - Whether to ask clarifying questions before research\n",
    "- **max_concurrent_research_units** - How many parallel researchers can run (default: 5)\n",
    "- **max_researcher_iterations** - How many times supervisor can delegate research (default: 6)\n",
    "- **max_react_tool_calls** - Tool call limit per researcher (default: 10)\n",
    "\n",
    "### Model Configuration\n",
    "- **research_model** - Model for research and supervision (we'll use Anthropic)\n",
    "- **compression_model** - Model for synthesizing findings\n",
    "- **final_report_model** - Model for writing the final report\n",
    "- **summarization_model** - Model for summarizing web search results\n",
    "\n",
    "### Search Configuration\n",
    "- **search_api** - Which search API to use (ANTHROPIC, TAVILY, or NONE)\n",
    "- **max_content_length** - Character limit before summarization\n",
    "\n",
    "Defined in [`open_deep_library/configuration.py`](open_deep_library/configuration.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration from the library\n",
    "from open_deep_library.configuration import (\n",
    "    Configuration,    # Lines 38-247: Main configuration class with all settings\n",
    "    SearchAPI,        # Lines 11-17: Enum for search API options (ANTHROPIC, TAVILY, NONE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Prompt Templates\n",
    "\n",
    "The system uses carefully engineered prompts for each phase:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "**clarify_with_user_instructions** - Analyzes if the research scope is clear or needs clarification\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "**transform_messages_into_research_topic_prompt** - Converts user messages into a detailed research brief\n",
    "\n",
    "### Phase 3: Supervisor\n",
    "**lead_researcher_prompt** - System prompt for the supervisor that manages delegation strategy\n",
    "\n",
    "### Phase 4: Researcher\n",
    "**research_system_prompt** - System prompt for individual researchers conducting focused research\n",
    "\n",
    "### Phase 5: Compression\n",
    "**compress_research_system_prompt** - Prompt for synthesizing research findings without losing information\n",
    "\n",
    "### Phase 6: Final Report\n",
    "**final_report_generation_prompt** - Comprehensive prompt for writing the final report\n",
    "\n",
    "All prompts are defined in [`open_deep_library/prompts.py`](open_deep_library/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt templates from the library\n",
    "from open_deep_library.prompts import (\n",
    "    clarify_with_user_instructions,                    # Lines 3-41: Ask clarifying questions\n",
    "    transform_messages_into_research_topic_prompt,     # Lines 44-77: Generate research brief\n",
    "    lead_researcher_prompt,                            # Lines 79-136: Supervisor system prompt\n",
    "    research_system_prompt,                            # Lines 138-183: Researcher system prompt\n",
    "    compress_research_system_prompt,                   # Lines 186-222: Research compression prompt\n",
    "    final_report_generation_prompt,                    # Lines 228-308: Final report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #1:\n",
    "\n",
    "Explain the interrelationships between the three states (Agent, Supervisor, Researcher). Why don't we just make a single huge state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "How the states relate\n",
    "\n",
    "Think of the system as one main agent that delegates to a supervisor, which in turn fans out work to many researchers.\n",
    "\n",
    "AgentState (top-level / ‚Äúproduct state‚Äù)\n",
    "\n",
    "Holds the user-facing conversation (messages) and final outputs (final_report)\n",
    "\n",
    "Also holds the shared research artifacts that flow across phases: research_brief, notes, raw_notes, plus supervisor_messages\n",
    "\n",
    "Runs the main phases: clarify ‚Üí write brief ‚Üí run supervisor ‚Üí write final report\n",
    "\n",
    "\n",
    "1. SupervisorState (coordination state)\n",
    "\n",
    "A subset focused on managing delegation and aggregation:\n",
    "\n",
    "a. supervisor_messages (the supervisor‚Äôs working thread)\n",
    "\n",
    "b. research_brief, notes, raw_notes\n",
    "\n",
    "c. research_iterations\n",
    "\n",
    "\n",
    "It decides what research units to run (via ConductResearch) and turns researcher outputs into notes for the final report.\n",
    "\n",
    "\n",
    "2. ResearcherState (worker state per research unit)\n",
    "\n",
    "One instance per delegated research topic.\n",
    "\n",
    "Tracks the researcher‚Äôs own tool loop state:\n",
    "\n",
    "a. researcher_messages\n",
    "\n",
    "b. tool_call_iterations\n",
    "\n",
    "c. research_topic\n",
    "\n",
    "d. compressed_research\n",
    "\n",
    "e. raw_notes\n",
    "\n",
    "\n",
    "Outputs a compact payload (compressed_research + notes) back to the supervisor.\n",
    "\n",
    "\n",
    "\n",
    "Why not a single huge state?\n",
    "\n",
    "1. Because each level has different responsibilities, different lifetimes, and different reducers, and mixing them creates real problems:\n",
    "\n",
    "2. Separation of concerns: Agent/Supervisor/Researcher need different fields. One giant state becomes a ‚Äújunk drawer‚Äù of fields most nodes don‚Äôt need.\n",
    "\n",
    "3. Prevents state collisions: You already have overlapping concepts (messages, notes). Splitting avoids confusing updates (e.g., researcher_messages vs supervisor_messages).\n",
    "\n",
    "4. Smaller context = better performance: A huge state increases what gets carried around and summarized. That worsens ‚Äúcontext rot‚Äù and can slow the system down.\n",
    "\n",
    "5. Cleaner reducers + safer updates: Your code uses custom reducers (like override_reducer) for some fields. Different graphs can apply the right reducer logic without side effects.\n",
    "\n",
    "6. Better scalability + parallelism: Supervisor can spawn many researchers concurrently. Separate ResearcherState instances keep each worker isolated.\n",
    "\n",
    "7. Easier debugging/testing: You can unit test supervisor behavior and researcher behavior independently.\n",
    "\n",
    "\n",
    "### In short: one huge state is harder to maintain, easier to break, and more expensive to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #2:\n",
    "\n",
    "What are the advantages and disadvantages of importing these components instead of including them in the notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "Advantages (why imports are good)\n",
    "\n",
    "1. Modularity & reuse: You can reuse state.py, prompts, and graph construction across notebooks/apps.\n",
    "\n",
    "2. Maintainability: Changes are localized (fix a prompt or state once, everywhere benefits).\n",
    "\n",
    "3. Testability: You can write unit tests for state reducers, router logic, researcher compression, etc.\n",
    "\n",
    "4. Cleaner notebook: The notebook focuses on how to use the system, not drowning in implementation detail.\n",
    "\n",
    "5. Production-ready structure: Matches real-world packaging (pyproject, library folder, versioning).\n",
    "\n",
    "Disadvantages (what you lose)\n",
    "\n",
    "1. Less transparent for learning: Readers can‚Äôt see everything in one place; they must jump between files.\n",
    "\n",
    "2. More setup friction: Imports/path issues (editable installs, working directory problems) can break notebooks.\n",
    "\n",
    "3. Harder to quick-edit: Prototyping is slightly slower if you constantly hop between modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Activity #1: Explore the Prompts\n",
    "\n",
    "Open `open_deep_library/prompts.py` and examine one of the prompt templates in detail.\n",
    "\n",
    "**Requirements:**\n",
    "1. Choose one prompt template (clarify, brief, supervisor, researcher, compression, or final report)\n",
    "2. Explain what the prompt is designed to accomplish\n",
    "3. Identify 2-3 key techniques used in the prompt (e.g., structured output, role definition, examples)\n",
    "4. Suggest one improvement you might make to the prompt\n",
    "\n",
    "**YOUR CODE HERE** - Write your analysis in a markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt chosen: lead_researcher_prompt (Supervisor prompt)\n",
    "\n",
    "1) What the prompt is designed to accomplish\n",
    "\n",
    "This prompt turns the model into a research supervisor whose main job is to:\n",
    "\n",
    "plan the research approach\n",
    "\n",
    "delegate research to sub-agents via the ConductResearch tool\n",
    "\n",
    "review progress after each research round\n",
    "\n",
    "stop at the right time by calling ResearchComplete\n",
    "\n",
    "\n",
    "In other words, it‚Äôs not meant to ‚Äúwrite the report.‚Äù It‚Äôs meant to manage the research process efficiently and decide when the research is ‚Äúgood enough‚Äù to move to final writing.\n",
    "\n",
    "2) 2‚Äì3 key techniques used in the prompt\n",
    "\n",
    "A) Strong role definition + constraints\n",
    "\n",
    "It clearly defines the role: ‚ÄúYou are a research supervisor.‚Äù\n",
    "\n",
    "It narrows the allowed actions to a small set of tools (ConductResearch, ResearchComplete, think_tool) so the agent behaves like a coordinator, not a free-form writer.\n",
    "\n",
    "\n",
    "B) Tool-first workflow with explicit control rules\n",
    "\n",
    "It mandates using think_tool before and after each ConductResearch call.\n",
    "\n",
    "It explicitly forbids parallel think_tool calls (‚ÄúDo not call think_tool with any other tools in parallel‚Äù), which reduces messy tool traces and keeps the loop predictable.\n",
    "\n",
    "\n",
    "C) Structured sections and step-by-step procedure\n",
    "\n",
    "The prompt uses labeled sections like <Task>, <Available Tools>, <Instructions>.\n",
    "\n",
    "It provides a numbered procedure that reduces ambiguity and improves consistency across runs.\n",
    "\n",
    "\n",
    "3) One improvement I would make\n",
    "\n",
    "Add a clear completion rubric so the supervisor knows exactly when to stop research, for example:\n",
    "\n",
    "‚ÄúStop when you have: (1) 5‚Äì8 high-quality sources, (2) coverage of all sub-questions, (3) at least one counterpoint or limitation, (4) a short evidence-backed outline.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü§ù Breakout Room #2\n",
    "## Building & Running the Researcher\n",
    "\n",
    "In this breakout room, we'll explore the node functions, build the graph, and run wellness research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Node Functions - The Building Blocks\n",
    "\n",
    "Now let's look at the node functions that make up our graph. We'll import them from the library and understand what each does.\n",
    "\n",
    "### The Complete Research Workflow\n",
    "\n",
    "The workflow consists of 8 key nodes organized into 3 subgraphs:\n",
    "\n",
    "1. **Main Graph Nodes:**\n",
    "   - `clarify_with_user` - Entry point that checks if clarification is needed\n",
    "   - `write_research_brief` - Transforms user input into structured research brief\n",
    "   - `final_report_generation` - Synthesizes all research into final report\n",
    "\n",
    "2. **Supervisor Subgraph Nodes:**\n",
    "   - `supervisor` - Lead researcher that plans and delegates\n",
    "   - `supervisor_tools` - Executes supervisor's tool calls (delegation, reflection)\n",
    "\n",
    "3. **Researcher Subgraph Nodes:**\n",
    "   - `researcher` - Individual researcher conducting focused research\n",
    "   - `researcher_tools` - Executes researcher's tool calls (search, reflection)\n",
    "   - `compress_research` - Synthesizes researcher's findings\n",
    "\n",
    "All nodes are defined in [`open_deep_library/deep_researcher.py`](open_deep_library/deep_researcher.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: clarify_with_user\n",
    "\n",
    "**Purpose:** Analyzes user messages and asks clarifying questions if the research scope is unclear.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check if clarification is enabled in configuration\n",
    "2. Use structured output to analyze if clarification is needed\n",
    "3. If needed, end with a clarifying question for the user\n",
    "4. If not needed, proceed to research brief with verification message\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 60-115](open_deep_library/deep_researcher.py#L60-L115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clarify_with_user node\n",
    "from open_deep_library.deep_researcher import clarify_with_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: write_research_brief\n",
    "\n",
    "**Purpose:** Transforms user messages into a structured research brief for the supervisor.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Use structured output to generate detailed research brief from messages\n",
    "2. Initialize supervisor with system prompt and research brief\n",
    "3. Set up supervisor messages with proper context\n",
    "\n",
    "**Why this matters:** A well-structured research brief helps the supervisor make better delegation decisions.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 118-175](open_deep_library/deep_researcher.py#L118-L175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_research_brief node\n",
    "from open_deep_library.deep_researcher import write_research_brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: supervisor\n",
    "\n",
    "**Purpose:** Lead research supervisor that plans research strategy and delegates to sub-researchers.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure model with three tools:\n",
    "   - `ConductResearch` - Delegate research to a sub-agent\n",
    "   - `ResearchComplete` - Signal that research is done\n",
    "   - `think_tool` - Strategic reflection before decisions\n",
    "2. Generate response based on current context\n",
    "3. Increment research iteration count\n",
    "4. Proceed to tool execution\n",
    "\n",
    "**Decision Making:** The supervisor uses `think_tool` to reflect before delegating research, ensuring thoughtful decomposition of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 178-223](open_deep_library/deep_researcher.py#L178-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor node (from supervisor subgraph)\n",
    "from open_deep_library.deep_researcher import supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: supervisor_tools\n",
    "\n",
    "**Purpose:** Executes the supervisor's tool calls, including strategic thinking and research delegation.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check exit conditions:\n",
    "   - Exceeded maximum iterations\n",
    "   - No tool calls made\n",
    "   - `ResearchComplete` called\n",
    "2. Process `think_tool` calls for strategic reflection\n",
    "3. Execute `ConductResearch` calls in parallel:\n",
    "   - Spawn researcher subgraphs for each delegation\n",
    "   - Limit to `max_concurrent_research_units` (default: 5)\n",
    "   - Gather all results asynchronously\n",
    "4. Aggregate findings and return to supervisor\n",
    "\n",
    "**Parallel Execution:** This is where the magic happens - multiple researchers work simultaneously on different aspects of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 225-349](open_deep_library/deep_researcher.py#L225-L349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor_tools node\n",
    "from open_deep_library.deep_researcher import supervisor_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: researcher\n",
    "\n",
    "**Purpose:** Individual researcher that conducts focused research on a specific topic.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load all available tools (search, MCP, reflection)\n",
    "2. Configure model with tools and researcher system prompt\n",
    "3. Generate response with tool calls\n",
    "4. Increment tool call iteration count\n",
    "\n",
    "**ReAct Pattern:** Researchers use `think_tool` to reflect after each search, deciding whether to continue or provide their answer.\n",
    "\n",
    "**Available Tools:**\n",
    "- Search tools (Tavily or Anthropic native search)\n",
    "- `think_tool` for strategic reflection\n",
    "- `ResearchComplete` to signal completion\n",
    "- MCP tools (if configured)\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 365-424](open_deep_library/deep_researcher.py#L365-L424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher node (from researcher subgraph)\n",
    "from open_deep_library.deep_researcher import researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 6: researcher_tools\n",
    "\n",
    "**Purpose:** Executes the researcher's tool calls, including searches and strategic reflection.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check early exit conditions (no tool calls, native search used)\n",
    "2. Execute all tool calls in parallel:\n",
    "   - Search tools fetch and summarize web content\n",
    "   - `think_tool` records strategic reflections\n",
    "   - MCP tools execute external integrations\n",
    "3. Check late exit conditions:\n",
    "   - Exceeded `max_react_tool_calls` (default: 10)\n",
    "   - `ResearchComplete` called\n",
    "4. Continue research loop or proceed to compression\n",
    "\n",
    "**Error Handling:** Safely handles tool execution errors and continues with available results.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 435-509](open_deep_library/deep_researcher.py#L435-L509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher_tools node\n",
    "from open_deep_library.deep_researcher import researcher_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 7: compress_research\n",
    "\n",
    "**Purpose:** Compresses and synthesizes research findings into a concise, structured summary.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure compression model\n",
    "2. Add compression instruction to messages\n",
    "3. Attempt compression with retry logic:\n",
    "   - If token limit exceeded, remove older messages\n",
    "   - Retry up to 3 times\n",
    "4. Extract raw notes from tool and AI messages\n",
    "5. Return compressed research and raw notes\n",
    "\n",
    "**Why Compression?** Researchers may accumulate lots of tool outputs and reflections. Compression ensures:\n",
    "- All important information is preserved\n",
    "- Redundant information is deduplicated\n",
    "- Content stays within token limits for the final report\n",
    "\n",
    "**Token Limit Handling:** Gracefully handles token limit errors by progressively truncating messages.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 511-585](open_deep_library/deep_researcher.py#L511-L585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compress_research node\n",
    "from open_deep_library.deep_researcher import compress_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 8: final_report_generation\n",
    "\n",
    "**Purpose:** Generates the final comprehensive research report from all collected findings.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extract all notes from completed research\n",
    "2. Configure final report model\n",
    "3. Attempt report generation with retry logic:\n",
    "   - If token limit exceeded, truncate findings by 10%\n",
    "   - Retry up to 3 times\n",
    "4. Return final report or error message\n",
    "\n",
    "**Token Limit Strategy:**\n",
    "- First retry: Use model's token limit √ó 4 as character limit\n",
    "- Subsequent retries: Reduce by 10% each time\n",
    "- Graceful degradation with helpful error messages\n",
    "\n",
    "**Report Quality:** The prompt guides the model to create well-structured reports with:\n",
    "- Proper headings and sections\n",
    "- Inline citations\n",
    "- Comprehensive coverage of all findings\n",
    "- Sources section at the end\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 607-697](open_deep_library/deep_researcher.py#L607-L697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final_report_generation node\n",
    "from open_deep_library.deep_researcher import final_report_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Graph Construction - Putting It All Together\n",
    "\n",
    "The system is organized into three interconnected graphs:\n",
    "\n",
    "### 1. Researcher Subgraph (Bottom Level)\n",
    "Handles individual focused research on a specific topic:\n",
    "```\n",
    "START ‚Üí researcher ‚Üí researcher_tools ‚Üí compress_research ‚Üí END\n",
    "               ‚Üë            ‚Üì\n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (loops until max iterations or ResearchComplete)\n",
    "```\n",
    "\n",
    "### 2. Supervisor Subgraph (Middle Level)\n",
    "Manages research delegation and coordination:\n",
    "```\n",
    "START ‚Üí supervisor ‚Üí supervisor_tools ‚Üí END\n",
    "            ‚Üë              ‚Üì\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (loops until max iterations or ResearchComplete)\n",
    "            \n",
    "supervisor_tools spawns multiple researcher_subgraphs in parallel\n",
    "```\n",
    "\n",
    "### 3. Main Deep Researcher Graph (Top Level)\n",
    "Orchestrates the complete research workflow:\n",
    "```\n",
    "START ‚Üí clarify_with_user ‚Üí write_research_brief ‚Üí research_supervisor ‚Üí final_report_generation ‚Üí END\n",
    "                 ‚Üì                                       (supervisor_subgraph)\n",
    "               (may end early if clarification needed)\n",
    "```\n",
    "\n",
    "Let's import the compiled graphs from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-compiled graphs from the library\n",
    "from open_deep_library.deep_researcher import (\n",
    "    # Bottom level: Individual researcher workflow\n",
    "    researcher_subgraph,    # Lines 588-605: researcher ‚Üí researcher_tools ‚Üí compress_research\n",
    "    \n",
    "    # Middle level: Supervisor coordination\n",
    "    supervisor_subgraph,    # Lines 351-363: supervisor ‚Üí supervisor_tools (spawns researchers)\n",
    "    \n",
    "    # Top level: Complete research workflow\n",
    "    deep_researcher,        # Lines 699-719: Main graph with all phases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Architecture?\n",
    "\n",
    "### Advantages of Supervisor-Researcher Delegation\n",
    "\n",
    "1. **Dynamic Task Decomposition**\n",
    "   - Unlike section-based approaches with predefined structure, the supervisor can break down research based on the actual question\n",
    "   - Adapts to different types of research (comparisons, lists, deep dives, etc.)\n",
    "\n",
    "2. **Parallel Execution**\n",
    "   - Multiple researchers work simultaneously on different aspects\n",
    "   - Much faster than sequential section processing\n",
    "   - Configurable parallelism (1-20 concurrent researchers)\n",
    "\n",
    "3. **ReAct Pattern for Quality**\n",
    "   - Researchers use `think_tool` to reflect after each search\n",
    "   - Prevents excessive searching and improves search quality\n",
    "   - Natural stopping conditions based on information sufficiency\n",
    "\n",
    "4. **Flexible Tool Integration**\n",
    "   - Easy to add MCP tools for specialized research\n",
    "   - Supports multiple search APIs (Anthropic, Tavily)\n",
    "   - Each researcher can use different tool combinations\n",
    "\n",
    "5. **Graceful Token Limit Handling**\n",
    "   - Compression prevents token overflow\n",
    "   - Progressive truncation in final report generation\n",
    "   - Research can scale to arbitrary depths\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Complexity:** More moving parts than section-based approach\n",
    "- **Cost:** Parallel researchers use more tokens (but faster)\n",
    "- **Unpredictability:** Research structure emerges dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Running the Deep Researcher\n",
    "\n",
    "Now let's see the system in action! We'll use it to research wellness strategies for improving sleep quality.\n",
    "\n",
    "### Setup\n",
    "\n",
    "We need to:\n",
    "1. Set up the wellness research request\n",
    "2. Configure the execution with Anthropic settings\n",
    "3. Run the research workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Graph ready for execution\n",
      "  (Note: The graph is pre-compiled from the library)\n"
     ]
    }
   ],
   "source": [
    "# Set up the graph with Anthropic configuration\n",
    "from IPython.display import Markdown, display\n",
    "import uuid\n",
    "\n",
    "# Note: deep_researcher is already compiled from the library\n",
    "# For this demo, we'll use it directly without additional checkpointing\n",
    "graph = deep_researcher\n",
    "\n",
    "print(\"‚úì Graph ready for execution\")\n",
    "print(\"  (Note: The graph is pre-compiled from the library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Anthropic\n",
    "\n",
    "We'll configure the system to use:\n",
    "- **Claude Sonnet 4** for all research, supervision, and report generation\n",
    "- **Tavily** for web search (you can also use Anthropic's native search)\n",
    "- **Moderate parallelism** (1 concurrent researcher for cost control)\n",
    "- **Clarification enabled** (will ask if research scope is unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 1\n",
      "  - Max Iterations: 2\n",
      "  - Search API: Tavily\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 8192,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 8192,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researcher\n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 3,           # Each researcher can make up to 3 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"tavily\",  # Using Tavily for web search\n",
    "        \"max_content_length\": 50000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úì Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 1\")\n",
    "print(f\"  - Max Iterations: 2\")\n",
    "print(f\"  - Search API: Tavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Wellness Research\n",
    "\n",
    "Now let's run the research! We'll ask the system to research evidence-based strategies for improving sleep quality.\n",
    "\n",
    "The workflow will:\n",
    "1. **Clarify** - Check if the request is clear (may skip if obvious)\n",
    "2. **Research Brief** - Transform our request into a structured brief\n",
    "3. **Supervisor** - Plan research strategy and delegate to researchers\n",
    "4. **Parallel Research** - Researchers gather information simultaneously\n",
    "5. **Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings combined into comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with your sleep improvement research request. I understand you're looking for evidence-based strategies to address your current sleep challenges, which include inconsistent bedtimes (10pm-1am), phone use in bed, and morning fatigue. I'll research the best scientific approaches to sleep hygiene and create a comprehensive, personalized sleep improvement plan that addresses these specific issues. I'll begin the research process now.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I want to improve my sleep quality and need a comprehensive, evidence-based sleep improvement plan. My current sleep challenges include: going to bed at inconsistent times (ranging from 10pm to 1am), using my phone in bed, and often feeling tired in the morning despite getting sleep. Please research the most effective, scientifically-backed strategies for improving sleep quality that specifically address inconsistent bedtime schedules, screen time before bed, and morning fatigue. I need a detail...\n",
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Evidence-Based Sleep Improvement Plan: Addressing Inconsistent Bedtimes, Screen Use, and Morning Fatigue\n",
       "\n",
       "Unfortunately, the research conducted for this comprehensive sleep improvement plan encountered technical limitations that prevented access to peer-reviewed studies and official clinical guidelines from major sleep organizations. However, based on established sleep science principles and clinical practice standards, the following evidence-based recommendations address the specific challenges of inconsistent bedtime schedules, screen time before bed, and morning fatigue.\n",
       "\n",
       "## Sleep Hygiene Foundation\n",
       "\n",
       "Optimal sleep hygiene forms the cornerstone of sleep quality improvement. Core practices include maintaining a cool bedroom temperature between 60-67¬∞F (15-19¬∞C), ensuring complete darkness through blackout curtains or eye masks, and minimizing noise disruption. The sleep environment should be reserved exclusively for sleep and intimate activities to strengthen the mental association between the bedroom and sleep.\n",
       "\n",
       "Regular physical activity, preferably completed at least 4-6 hours before bedtime, significantly improves sleep quality and reduces the time needed to fall asleep. However, vigorous exercise close to bedtime can be stimulating and should be avoided. Similarly, caffeine consumption should be limited after 2 PM, as caffeine has a half-life of 5-6 hours and can interfere with sleep initiation and depth even when consumed earlier in the day.\n",
       "\n",
       "## Establishing Consistent Sleep Timing\n",
       "\n",
       "The current 3-hour variation in bedtime (10 PM to 1 AM) represents a significant circadian rhythm disruption that requires systematic correction. Circadian rhythm entrainment relies on consistent light-dark cycles and regular sleep-wake timing to optimize melatonin production and core body temperature fluctuations.\n",
       "\n",
       "To establish consistency, gradually shift bedtime by 15-30 minutes earlier each night until reaching the target bedtime. This process should take 1-2 weeks to avoid shocking the circadian system. Once the desired bedtime is achieved, maintain it within a 30-minute window every night, including weekends, to prevent \"social jet lag.\"\n",
       "\n",
       "Morning light exposure within the first hour of waking serves as the most powerful circadian anchor. Spend 15-30 minutes outdoors or near a bright window immediately upon waking, even on cloudy days. This light exposure suppresses residual melatonin and signals the circadian clock to maintain proper sleep-wake timing.\n",
       "\n",
       "## Screen Time and Blue Light Management\n",
       "\n",
       "Electronic device usage before bedtime disrupts sleep through multiple mechanisms: blue light exposure suppresses melatonin production, cognitive stimulation increases arousal, and the content consumed can trigger emotional responses that interfere with the relaxation necessary for sleep onset.\n",
       "\n",
       "Implement a digital sunset by ceasing all screen use 1-2 hours before the target bedtime. This includes smartphones, tablets, computers, and television. If complete avoidance is impossible, utilize blue light filtering glasses or device settings that reduce blue light emission after sunset. However, these measures are less effective than complete screen avoidance.\n",
       "\n",
       "Create a charging station outside the bedroom to eliminate the temptation to use devices in bed. Replace bedtime phone use with relaxing activities such as reading physical books, gentle stretching, meditation, or journaling. These alternative activities promote the relaxation response needed for quality sleep.\n",
       "\n",
       "## Addressing Morning Fatigue\n",
       "\n",
       "Persistent morning fatigue despite adequate sleep duration often indicates poor sleep quality rather than insufficient sleep quantity. Sleep architecture consists of multiple stages, including deep sleep (slow-wave sleep) and REM sleep, both crucial for feeling refreshed upon waking.\n",
       "\n",
       "Sleep inertia, the groggy feeling upon waking, can be minimized by avoiding sleep fragmentation and ensuring complete sleep cycles. A typical sleep cycle lasts 90-110 minutes, so timing total sleep to align with complete cycles (7.5 or 9 hours rather than 8 hours) may reduce morning grogginess.\n",
       "\n",
       "Consistent wake times, even more important than consistent bedtimes, help regulate circadian rhythms and reduce morning fatigue. Set an alarm for the same time every day, including weekends, and resist the urge to hit the snooze button, which fragments sleep and increases grogginess.\n",
       "\n",
       "## Pre-Sleep Routine Development\n",
       "\n",
       "Establish a consistent 30-60 minute wind-down routine that signals to the body that sleep is approaching. This routine should be relaxing and the same each night to create a conditioned response. Effective activities include taking a warm bath or shower (which facilitates the drop in core body temperature needed for sleep), practicing progressive muscle relaxation, or engaging in gentle yoga or stretching.\n",
       "\n",
       "Temperature regulation plays a crucial role in sleep onset. The body needs to drop its core temperature by 1-2 degrees Fahrenheit to initiate sleep. A warm bath 90 minutes before bedtime facilitates this temperature drop through vasodilation when exiting the warm water.\n",
       "\n",
       "## Advanced Sleep Optimization Strategies\n",
       "\n",
       "Sleep restriction therapy can improve sleep efficiency by limiting time in bed to match actual sleep time, then gradually increasing as sleep quality improves. This technique should be implemented carefully and may benefit from professional guidance.\n",
       "\n",
       "Stimulus control involves strengthening the association between the bedroom and sleep by following specific rules: only go to bed when sleepy, leave the bed if unable to fall asleep within 15-20 minutes, and return only when sleepy again. This prevents the bed from becoming associated with wakefulness and frustration.\n",
       "\n",
       "Relaxation techniques such as progressive muscle relaxation, diaphragmatic breathing, and mindfulness meditation can reduce pre-sleep arousal and racing thoughts that interfere with sleep onset. These techniques require practice to become effective but can significantly improve sleep quality over time.\n",
       "\n",
       "## Implementation Strategy\n",
       "\n",
       "Begin implementing changes gradually to avoid overwhelming existing routines. Start with establishing a consistent bedtime and eliminating screen use in the bedroom, as these address the most significant current sleep disruptors. Once these habits are established (typically 2-3 weeks), add additional sleep hygiene practices and optimization strategies.\n",
       "\n",
       "Track sleep patterns and quality using a sleep diary to identify which interventions provide the greatest benefit. Note bedtime, wake time, estimated sleep onset time, number of awakenings, and morning energy levels to objectively assess progress.\n",
       "\n",
       "Consider professional evaluation if morning fatigue persists despite implementing these strategies, as underlying sleep disorders such as sleep apnea, restless leg syndrome, or other medical conditions may require specific treatment beyond behavioral interventions.\n",
       "\n",
       "### Sources\n",
       "\n",
       "No sources were successfully retrieved during this research session due to technical limitations. This comprehensive plan is based on established sleep science principles and clinical practice standards, but peer-reviewed sources from sleep medicine journals and official sleep organization guidelines could not be accessed as initially intended."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our wellness research request\n",
    "research_request = \"\"\"\n",
    "I want to improve my sleep quality. I currently:\n",
    "- Go to bed at inconsistent times (10pm-1am)\n",
    "- Use my phone in bed\n",
    "- Often feel tired in the morning\n",
    "\n",
    "Please research the best evidence-based strategies for improving sleep quality and create a comprehensive sleep improvement plan for me.\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Understanding the Output\n",
    "\n",
    "Let's break down what happened:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "The system checked if your request was clear. Since you provided specific details about your sleep issues, it likely proceeded without asking clarifying questions.\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "Your request was transformed into a detailed research brief that guides the supervisor's delegation strategy.\n",
    "\n",
    "### Phase 3: Supervisor Delegation\n",
    "The supervisor analyzed the brief and decided how to break down the research:\n",
    "- Used `think_tool` to plan strategy\n",
    "- Called `ConductResearch` to delegate to researchers\n",
    "- Each delegation specified a focused research topic (e.g., sleep hygiene, circadian rhythm, blue light effects)\n",
    "\n",
    "### Phase 4: Parallel Research\n",
    "Researchers worked on their assigned topics:\n",
    "- Each researcher used web search tools to gather information\n",
    "- Used `think_tool` to reflect after each search\n",
    "- Decided when they had enough information\n",
    "- Compressed their findings into clean summaries\n",
    "\n",
    "### Phase 5: Final Report\n",
    "All research findings were synthesized into a comprehensive sleep improvement plan with:\n",
    "- Well-structured sections\n",
    "- Evidence-based recommendations\n",
    "- Practical action items\n",
    "- Sources for further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Key Takeaways & Next Steps\n",
    "\n",
    "### Architecture Benefits\n",
    "1. **Dynamic Decomposition** - Research structure emerges from the question, not predefined\n",
    "2. **Parallel Efficiency** - Multiple researchers work simultaneously\n",
    "3. **ReAct Quality** - Strategic reflection improves search decisions\n",
    "4. **Scalability** - Handles token limits gracefully through compression\n",
    "5. **Flexibility** - Easy to add new tools and capabilities\n",
    "\n",
    "### When to Use This Pattern\n",
    "- **Complex research questions** that need multi-angle investigation\n",
    "- **Comparison tasks** where parallel research on different topics is beneficial\n",
    "- **Open-ended exploration** where structure should emerge dynamically\n",
    "- **Time-sensitive research** where parallel execution speeds up results\n",
    "\n",
    "### When to Use Section-Based Instead\n",
    "- **Highly structured reports** with predefined format requirements\n",
    "- **Template-based content** where sections are always the same\n",
    "- **Sequential dependencies** where later sections depend on earlier ones\n",
    "- **Budget constraints** where token efficiency is critical\n",
    "\n",
    "### Extend the System\n",
    "1. **Add MCP Tools** - Integrate specialized tools for your domain\n",
    "2. **Custom Prompts** - Modify prompts for specific research types\n",
    "3. **Different Models** - Try different Claude versions or mix models\n",
    "4. **Persistence** - Use a real database for checkpointing instead of memory\n",
    "\n",
    "### Learn More\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Open Deep Research Repo](https://github.com/langchain-ai/open_deep_research)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Tavily Search API](https://tavily.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #3:\n",
    "\n",
    "What are the trade-offs of using parallel researchers vs. sequential research? When might you choose one approach over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "Parallel research (many researchers at once)\n",
    "\n",
    "## Pros\n",
    "\n",
    "1. Much faster wall-clock time (great when you need breadth quickly)\n",
    "\n",
    "2. Better coverage: different angles/sources in one round\n",
    "\n",
    "3. Works well when tasks are independent (e.g., ‚Äúbenefits‚Äù, ‚Äúrisks‚Äù, ‚Äúbest practices‚Äù, ‚Äúlatest research‚Äù)\n",
    "\n",
    "\n",
    "## Cons\n",
    "\n",
    "1. Higher cost (multiple model calls + tools)\n",
    "\n",
    "2. More noisy/duplicative outputs; harder to merge\n",
    "\n",
    "3. More risk of inconsistent assumptions or contradictions\n",
    "\n",
    "4. Harder to manage context: more notes to summarize (‚Äúcontext rot‚Äù)\n",
    "\n",
    "\n",
    "\n",
    "Sequential research (one researcher/round at a time)\n",
    "\n",
    "## Pros\n",
    "\n",
    "1. Cheaper and easier to control\n",
    "\n",
    "2. Each step can build on the last (refine questions, fill gaps)\n",
    "\n",
    "3. Cleaner, less duplication, simpler aggregation\n",
    "\n",
    "\n",
    "## Cons\n",
    "\n",
    "1. Slower end-to-end\n",
    "\n",
    "2. Can miss breadth unless you do many rounds\n",
    "\n",
    "\n",
    "\n",
    "When to choose which\n",
    "\n",
    "## Choose parallel when:\n",
    "\n",
    "1. you have a fixed deadline / need speed\n",
    "\n",
    "2. you want broad coverage across subtopics\n",
    "\n",
    "3. sources are independent (no need for iterative refinement)\n",
    "\n",
    "## Choose sequential when:\n",
    "\n",
    "1. budget is tight\n",
    "\n",
    "2. the question is ambiguous and needs iterative narrowing\n",
    "\n",
    "3. you need high reliability and careful synthesis (fewer moving parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question #4:\n",
    "\n",
    "How would you adapt this deep research architecture for a production wellness application? What additional components would you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer:\n",
    "To productionize it for wellness, you‚Äôd keep the same roles (Agent ‚Üí Supervisor ‚Üí Researchers) but add safety, persistence, and operations.\n",
    "\n",
    "1) Safety guardrails (critical)\n",
    "\n",
    "Medical safety policy: no diagnosis, no medication changes, escalation rules for red flags\n",
    "\n",
    "Contraindication checks using user profile (allergies, meds, conditions)\n",
    "\n",
    "Tool gating: restrict web search, filter sources, block untrusted sites\n",
    "\n",
    "Safety evaluations + red teaming, plus automated ‚Äúunsafe advice‚Äù detection\n",
    "\n",
    "\n",
    "2) Persistent storage + user isolation\n",
    "\n",
    "Store user profile, preferences, and conditions in a real DB (Postgres)\n",
    "\n",
    "Store daily check-ins, plans, and summaries in durable storage (S3/object store)\n",
    "\n",
    "Namespace everything by tenant_id/user_id to prevent leakage\n",
    "\n",
    "Keep an audit trail of what was recommended and why (for trust + debugging)\n",
    "\n",
    "\n",
    "3) Context management at scale\n",
    "\n",
    "Retrieval over curated health content (your KB) + optionally vetted web\n",
    "\n",
    "Summarization layers:\n",
    "\n",
    "user snapshot (goals, constraints, current plan)\n",
    "\n",
    "rolling weekly summary\n",
    "\n",
    "\n",
    "Don‚Äôt pass full history; pass structured state + retrieved snippets\n",
    "\n",
    "\n",
    "4) Observability + monitoring\n",
    "\n",
    "Tracing per run: which researchers fired, tool calls, timings, failures\n",
    "\n",
    "Metrics: latency, token usage, cost per request, cache hit rate, safety event counts\n",
    "\n",
    "Quality monitoring: user feedback, outcome tracking (did they follow plan? improvements?)\n",
    "\n",
    "\n",
    "5) Cost controls\n",
    "\n",
    "Budget limits per request (max tool calls, max researchers, max tokens)\n",
    "\n",
    "Adaptive parallelism (only fan out when needed)\n",
    "\n",
    "Caching (semantic cache for repeated queries, cached retrieval results)\n",
    "\n",
    "Model tiering: cheaper models for extraction/formatting, stronger for supervision/safety synthesis\n",
    "\n",
    "\n",
    "6) Product features\n",
    "\n",
    "Personalization engine using stored preferences + constraints\n",
    "\n",
    "Scheduling/reminders (optional) and progress tracking\n",
    "\n",
    "Human-in-the-loop escalation for high-risk or ambiguous cases\n",
    "\n",
    "\n",
    "Net: the research architecture is a great backbone, but production requires guardrails, persistence, isolation, observability, and cost governance on top.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Activity #2: Custom Wellness Research\n",
    "\n",
    "Using what you've learned, run a custom wellness research task.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create a wellness-related research question (exercise, nutrition, stress, etc.)\n",
    "2. Modify the configuration for your use case\n",
    "3. Run the research and analyze the output\n",
    "4. Document what worked well and what could be improved\n",
    "\n",
    "**Experiment ideas:**\n",
    "- Research exercise routines for specific conditions (bad knee, lower back pain)\n",
    "- Compare different stress management techniques\n",
    "- Investigate nutrition strategies for specific goals\n",
    "- Explore meditation and mindfulness research\n",
    "\n",
    "**YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SinhaK\\AppData\\Local\\miniconda3\\envs\\agentenv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Research plan created.\n",
      "Reflection recorded: Task: Compare evidence-based stress management techniques for working professionals. Focus on 5 strategies: mindfulness meditation, paced breathing, exercise, CBT-based journaling/reframing, and sleep hygiene.\n",
      "\n",
      "Create a short research plan:\n",
      "- Sub-questions to answer\n",
      "- Keywords to search\n",
      "- Evidence to prioritize (guidelines, systematic reviews, RCTs)\n",
      "- Final report outline\n",
      "Keep it concise.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tavily_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Search round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG[\u001b[33m'\u001b[39m\u001b[33msearch_rounds\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_findings\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m all_findings = \u001b[38;5;28;01mawait\u001b[39;00m run_search_rounds()\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 3) Synthesis\u001b[39;00m\n\u001b[32m     92\u001b[39m synthesis = think_tool.invoke({\n\u001b[32m     93\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreflection\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m     94\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing the research notes below, write a structured markdown report with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     )\n\u001b[32m    104\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mrun_search_rounds\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m3\u001b[39m:\n\u001b[32m     79\u001b[39m     q += \u001b[33m\"\u001b[39m\u001b[33m randomized trial workplace stress\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m res = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mtavily_search\u001b[49m.ainvoke({\n\u001b[32m     82\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mqueries\u001b[39m\u001b[33m\"\u001b[39m: [q],\n\u001b[32m     83\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_results\u001b[39m\u001b[33m\"\u001b[39m: CFG[\u001b[33m\"\u001b[39m\u001b[33mresults_per_round\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     84\u001b[39m })\n\u001b[32m     85\u001b[39m all_findings.append(res)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Search round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG[\u001b[33m'\u001b[39m\u001b[33msearch_rounds\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tavily_search' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Final Activity #2 ‚Äî Custom Wellness Research (Jupyter-safe: top-level await)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "RESEARCH_QUESTION = (\n",
    "    \"Compare evidence-based stress management techniques for working professionals. \"\n",
    "    \"Focus on 5 strategies: (1) mindfulness meditation, (2) paced breathing, \"\n",
    "    \"(3) exercise, (4) CBT-based journaling/reframing, (5) sleep hygiene. \"\n",
    "    \"For each: mechanism, how to do it, time required, who it works best for, limitations.\"\n",
    ")\n",
    "\n",
    "CFG = {\n",
    "    \"search_rounds\": 3,\n",
    "    \"results_per_round\": 5,\n",
    "    \"save_dir\": Path(\"workspace\") / \"custom_research\",\n",
    "    \"report_filename\": \"stress_management_comparison.md\",\n",
    "}\n",
    "\n",
    "CFG[\"save_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "today = get_today_str() if \"get_today_str\" in globals() else datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def save_md(path: Path, content: str) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "# ============================================================\n",
    "# One-shot setup: import tools + run think_tool plan\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure current folder is importable (contains open_deep_library/)\n",
    "ROOT = Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Safe imports (does NOT require open_deep_research to exist)\n",
    "from open_deep_library.utils import (\n",
    "    think_tool,\n",
    "    get_today_str,\n",
    ")\n",
    "\n",
    "# ---- Define your research question ----\n",
    "RESEARCH_QUESTION = (\n",
    "    \"Compare evidence-based stress management techniques for working professionals. \"\n",
    "    \"Focus on 5 strategies: mindfulness meditation, paced breathing, exercise, \"\n",
    "    \"CBT-based journaling/reframing, and sleep hygiene.\"\n",
    ")\n",
    "\n",
    "# ---- Run plan (think_tool expects 'reflection') ----\n",
    "plan = think_tool.invoke({\n",
    "    \"reflection\": (\n",
    "        f\"Task: {RESEARCH_QUESTION}\\n\\n\"\n",
    "        \"Create a short research plan:\\n\"\n",
    "        \"- Sub-questions to answer\\n\"\n",
    "        \"- Keywords to search\\n\"\n",
    "        \"- Evidence to prioritize (guidelines, systematic reviews, RCTs)\\n\"\n",
    "        \"- Final report outline\\n\"\n",
    "        \"Keep it concise.\"\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Research plan created.\")\n",
    "print(plan)\n",
    "\n",
    "\n",
    "# 2) Async Tavily search rounds (tavily_search is async-only)\n",
    "async def run_search_rounds():\n",
    "    all_findings = []\n",
    "    for i in range(1, CFG[\"search_rounds\"] + 1):\n",
    "        q = RESEARCH_QUESTION\n",
    "        if i == 2:\n",
    "            q += \" systematic review meta-analysis\"\n",
    "        if i == 3:\n",
    "            q += \" randomized trial workplace stress\"\n",
    "\n",
    "        res = await tavily_search.ainvoke({\n",
    "            \"queries\": [q],\n",
    "            \"max_results\": CFG[\"results_per_round\"],\n",
    "        })\n",
    "        all_findings.append(res)\n",
    "        print(f\"‚úÖ Search round {i}/{CFG['search_rounds']} complete.\")\n",
    "    return all_findings\n",
    "\n",
    "all_findings = await run_search_rounds()\n",
    "\n",
    "# 3) Synthesis\n",
    "synthesis = think_tool.invoke({\n",
    "    \"reflection\": (\n",
    "        \"Using the research notes below, write a structured markdown report with:\\n\"\n",
    "        \"1) Executive summary\\n\"\n",
    "        \"2) Comparison table (strategy, time, difficulty, evidence strength, best-for)\\n\"\n",
    "        \"3) 5 detailed sections (mechanism, protocol, expected benefits, limitations)\\n\"\n",
    "        \"4) Practical 2-week starter plan combining 2‚Äì3 techniques\\n\"\n",
    "        \"5) References list (use sources mentioned in the notes)\\n\"\n",
    "        \"Avoid medical diagnosis. Be clear and actionable.\\n\\n\"\n",
    "        f\"RESEARCH QUESTION:\\n{RESEARCH_QUESTION}\\n\\n\"\n",
    "        f\"RESEARCH NOTES:\\n{all_findings}\"\n",
    "    )\n",
    "})\n",
    "\n",
    "report_md = f\"\"\"# Custom Wellness Research Report\n",
    "**Date:** {today}  \n",
    "**Question:** {RESEARCH_QUESTION}\n",
    "\n",
    "---\n",
    "\n",
    "## Research plan\n",
    "{plan}\n",
    "\n",
    "---\n",
    "\n",
    "## Findings + synthesis\n",
    "{synthesis}\n",
    "\"\"\"\n",
    "\n",
    "out_path = CFG[\"save_dir\"] / CFG[\"report_filename\"]\n",
    "save_md(out_path, report_md)\n",
    "print(f\"\\n‚úÖ Saved report to: {out_path}\")\n",
    "\n",
    "# 4) Evaluate\n",
    "analysis = think_tool.invoke({\n",
    "    \"reflection\": (\n",
    "        \"Evaluate the report quality in ~10 bullets:\\n\"\n",
    "        \"- What worked well (coverage, structure, evidence quality)\\n\"\n",
    "        \"- What could be improved (missing angles, weak evidence, unclear parts)\\n\"\n",
    "        \"- What config changes to try next\\n\\n\"\n",
    "        f\"REPORT CONTENT:\\n{synthesis}\"\n",
    "    )\n",
    "})\n",
    "\n",
    "analysis_path = CFG[\"save_dir\"] / \"analysis_notes.md\"\n",
    "save_md(analysis_path, f\"# Output evaluation\\n\\n{analysis}\\n\")\n",
    "print(f\"‚úÖ Saved analysis to: {analysis_path}\")\n",
    "\n",
    "print(\"\\n--- Preview (first 15 lines) ---\")\n",
    "for line in report_md.splitlines()[:15]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
