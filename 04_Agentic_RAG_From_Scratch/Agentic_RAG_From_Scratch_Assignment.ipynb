{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Agentic RAG From Scratch: Building with LangGraph and Open-Source Models\n",
    "\n",
    "In this notebook, we'll look under the hood of `create_agent` and build an agentic RAG application **from scratch** using LangGraph's low-level primitives and locally-hosted open-source models.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand LangGraph's core constructs: StateGraph, nodes, edges, and conditional routing\n",
    "- Build a ReAct agent from scratch without high-level abstractions\n",
    "- Use Ollama to run open-source models locally (gpt-oss:20b + embeddinggemma)\n",
    "- Transition from `aimakerspace` utilities to the LangChain ecosystem\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- **Breakout Room #1:** LangGraph Fundamentals & Building Agents from Scratch\n",
    "  - Task 1: Dependencies & Ollama Setup\n",
    "  - Task 2: LangGraph Core Concepts (StateGraph, Nodes, Edges)\n",
    "  - Task 3: Building a ReAct Agent from Scratch\n",
    "  - Task 4: Adding Tools to Your Agent\n",
    "  - Question #1 & Question #2\n",
    "  - Activity #1: Implement a Custom Routing Function\n",
    "\n",
    "- **Breakout Room #2:** Agentic RAG with Local Models\n",
    "  - Task 5: Loading & Chunking with LangChain\n",
    "  - Task 6: Setting up Qdrant with Local Embeddings\n",
    "  - Task 7: Creating a RAG Tool\n",
    "  - Task 8: Building Agentic RAG from Scratch\n",
    "  - Question #3 & Question #4\n",
    "  - Activity #2: Extend the Agent with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "# Breakout Room #1\n",
    "## LangGraph Fundamentals & Building Agents from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Task 1: Dependencies & Ollama Setup\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "1. **Ollama installed** - Download from [ollama.com](https://ollama.com/)\n",
    "2. **Ollama running** - Start with `ollama serve` in a terminal\n",
    "3. **Models pulled** - Run these commands:\n",
    "\n",
    "```bash\n",
    "# Chat model for reasoning and generation (~12GB)\n",
    "ollama pull gpt-oss:20b\n",
    "\n",
    "# Embedding model for RAG (~622MB)\n",
    "ollama pull embeddinggemma\n",
    "```\n",
    "\n",
    "> **Note**: If you don't have enough RAM/VRAM for `gpt-oss:20b` (requires 16GB+ VRAM or 24GB+ RAM), you can substitute with `llama3.2:3b` or another smaller model.\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [Ollama Installation Guide](https://ollama.com/download)\n",
    "- [gpt-oss Model Card](https://ollama.com/library/gpt-oss)\n",
    "- [EmbeddingGemma Model Card](https://ollama.com/library/embeddinggemma)\n",
    "- [langchain-ollama Integration](https://python.langchain.com/docs/integrations/providers/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports we'll use throughout the notebook\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from typing import Annotated, TypedDict, Literal\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Required for async operations in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ff930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693693dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing enabled. Project: AIE9 - The Agent Loop - eb4f6511\n"
     ]
    }
   ],
   "source": [
    "# Optional: Set up LangSmith for tracing\n",
    "# This provides powerful debugging and observability for your agents\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE9 - The Agent Loop - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key (press Enter to skip): \") or \"\"\n",
    "\n",
    "if not os.environ[\"LANGCHAIN_API_KEY\"]:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    print(\"LangSmith tracing disabled\")\n",
    "else:\n",
    "    print(f\"LangSmith tracing enabled. Project: {os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify Ollama is running and models are available\n",
    "# from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# # Test connection to Ollama\n",
    "# try:\n",
    "#     test_llm = ChatOllama(model=\"gpt-oss:20b\", temperature=0)\n",
    "#     test_response = test_llm.invoke(\"Say 'Ollama is working!' in exactly 3 words.\")\n",
    "#     print(f\"Chat Model Test: {test_response.content}\")\n",
    "    \n",
    "#     test_embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "#     test_vector = test_embeddings.embed_query(\"test\")\n",
    "#     print(f\"Embedding Model Test: Vector dimension = {len(test_vector)}\")\n",
    "#     print(\"\\nOllama is ready!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error connecting to Ollama: {e}\")\n",
    "#     print(\"\\nMake sure:\")\n",
    "#     print(\"1. Ollama is installed: https://ollama.com/\")\n",
    "#     print(\"2. Ollama is running: 'ollama serve'\")\n",
    "#     print(\"3. Models are pulled: 'ollama pull gpt-oss:20b' and 'ollama pull embeddinggemma'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Task 2: LangGraph Core Concepts\n",
    "\n",
    "In Session 3, we used `create_agent` which abstracts away the complexity. Now let's understand what's happening under the hood!\n",
    "\n",
    "### LangGraph models workflows as **graphs** with three key components:\n",
    "\n",
    "### 1. State\n",
    "A shared data structure that represents the current snapshot of your application:\n",
    "\n",
    "```python\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # Conversation history\n",
    "```\n",
    "\n",
    "The `add_messages` **reducer** ensures new messages are appended (not replaced) when the state updates.\n",
    "\n",
    "### 2. Nodes\n",
    "Python functions that encode the logic of your agent:\n",
    "- Receive the current state\n",
    "- Perform computation or side-effects\n",
    "- Return an updated state\n",
    "\n",
    "### 3. Edges\n",
    "Functions that determine which node to execute next:\n",
    "- **Normal edges**: Always go to a specific node\n",
    "- **Conditional edges**: Choose the next node based on state\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [LangGraph Low-Level Concepts](https://langchain-ai.github.io/langgraph/concepts/low_level/)\n",
    "- [LangGraph Quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/)\n",
    "- [StateGraph API Reference](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple echo graph created!\n"
     ]
    }
   ],
   "source": [
    "# Let's build our first LangGraph workflow - a simple echo graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Step 1: Define the State\n",
    "class SimpleState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Step 2: Define Nodes (functions that process state)\n",
    "def echo_node(state: SimpleState):\n",
    "    \"\"\"A simple node that echoes the last message.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    echo_response = AIMessage(content=f\"You said: {last_message.content}\")\n",
    "    return {\"messages\": [echo_response]}\n",
    "\n",
    "# Step 3: Build the Graph\n",
    "echo_graph = StateGraph(SimpleState)\n",
    "\n",
    "# Add nodes\n",
    "echo_graph.add_node(\"echo\", echo_node)\n",
    "\n",
    "# Add edges (START -> echo -> END)\n",
    "echo_graph.add_edge(START, \"echo\")\n",
    "echo_graph.add_edge(\"echo\", END)\n",
    "\n",
    "# Compile the graph\n",
    "echo_app = echo_graph.compile()\n",
    "\n",
    "print(\"Simple echo graph created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAADqCAIAAADF80cYAAAQAElEQVR4nOydCXwT1b7Hz0z2pPsS2qYt3aFFoEBZallkRx9YQEQey/WKqFR2BJ8gF18BL6KCoIKIPMQPF3ABEQREQDYLChUKtpQC3enedEmbpdkm7ySBkLaTradpJ3S++inJWSaTX87yP9v8mTqdDtC0FSagQYCWDwlaPiRo+ZCg5UOClg8JVPkKbytzMxpqq5VNMo2OwEBzKwhjAJ0W/tXptJgxRIcB7FEanIkRGp0pmVkufXoMBzricSC8Nq4DLQKh0YVhLbPjTEBojK9gNsMFcZ3+3h7BFjBYLIzvzuge69Yr0Q0ggLXN7rv+myTrSr1UooF3D2+FwcJwBsZgYjpts6thDH2ISaYW4CyMUBvkwzEdoTMLxwk10SIQw3Ed0TIQQE10LbMzWLhWbZQNA4Zvh8EbM7sBDo+pVhFKhVathr+TjsNnhPcSjJzuDxzHYfkyfpOk/1ZDaIF/MGfgGL/QWA5wZaRicOnnitJcqCUR3tt9/ByhQ9kdk++b9YUKKRE3xGv4FB/wZJFzVXblZLVWS7z2XgRg2ZvLAfm+WJHnH8KdtkQEnlwuHhbDRinpef/4EZ72pLdXvs+X546eHhA7BKmhdRV2rMibsyrM3ZdhM6Vd8u1YmTcvNZLNB12HL/8nL2Gs34AxNsogDmyx8+28kS9261LaQd7YFPnnKbGkWmM9mQ35vllXBNu72EFdos62YMgEv2+3FFtPY02+62frlXLihUVPcl9hBVhzuXzG4U9LraSxKt+5ul6JXqALM21JSHmhwkoCi/Ldutig1eiSkr1BF0bggQs8mEe2l1lKYFG+jIt1QlFHjyjGjh1bWlrqaK68vLyJEycC59A7ybOiyGIBtCifTKIZNMEPdCDl5eV1dXXAcbKzs4HTSBjrDU274rvkCpLPuORmyOA4PKSHU0oftDQPHjx4/PjxoqKi8PDwIUOGpKSkZGRkzJ8/H8YmJyePGDFi8+bNsEwdOnQoPT29rKwsIiJi8uTJ06ZNM15h9OjR8+bNO3fuHMw1Z86cffv26b9nQsKyZctmzZoF2hsuH8/6XRLag9c6ily+gtsyNhcDzuHbb7/ds2fP0qVLk5KSLly4sH37doFA8Morr2zduhUGHj16VCTS9/VQQSjcu+++i2FYYWHhpk2bAgMDYRYYxWKxjhw5MmjQICjigAEDYILTp0/D3wM4BzdPVm21ijSKXD5JjZrDs21Rt40bN27ExcUZW6spU6YMHDhQLpe3TrZx40aZTBYUFAQMJevYsWNXrlwxygf18vT0XLFiBegQPP3ZpfflpFHk8qlVWjbbWfL17dv3s88+W7duXb9+/YYPHx4cHEyaDNZxWE4vX74M67gxxFgqjcAfAHQUXAGuVmtJo8jl02oIjOUs+WbOnAlr68WLF1NTU5lMJuxtFy9e7O/fbLaSIIglS5aoVKqFCxfCoufu7v7qq6+aJ2Cz2aCj0E9oY+RNGbl8cD4WjjeAc8BxfIqB/Pz8a9eu7dq1SyqVfvLJJ+ZpcnJybt++vWPHDtjAGUMaGxuFQsfmMtsLRSOBOySfuxe7oZa8tqMD2/jY2NjIyMgIA1AX2A+0SFNfXw//mvTKNwCzgM6goUbN4pFPXpHXUFEMDy79AOdw6tSplStXXrp0SSKRpKWlQfsDtoYwPCwsDP49c+ZMVlYWlBXWa2iRNDQ0wG73o48+gvYNNAxJLxgaGioWi2Enbmol2xdJrcrTm3wCmly+3k+7wwUtcRl5b43ImjVroDrLly+H5tv69euhlQetExgO+5BJkybt3LkTdiwBAQEbNmzIzMwcNWoUtOYWLFgAjT4oq8n0M2fo0KHx8fGwI/7111+BE2iSaXsmkM85WZwu/erdfGEwNzklCHRt7lyTnfuufMHmKNJYi91rTH/3klxnNX8uxF9nxD4BFkdfFpfJR7zgf/sPyc0LkvhnyCesKyoqZsyYQRrl5uYGO1PSKFht4ZADOIe9BkijoOVhqZ5B24i0TTBSL1a9tiHKUqy1tY6zB6ru32xM+ZC8v9NoNFVVVaRRTU1NXC6XNAp2CM6zPxoNkEbBLsjDw4M0CobD35s06uCHDwitbtaqUGABG0tFu1YXdI/lj5/TDXQ9iu82/byrxFKrZ8TG0OL1f4fn3pQ2NTjLhKYyJ3aXDp1so6LYHpmNm9lt7/sFoIvx9f8WhfZw6zvMw3oyu9Z5aytUBz4sXrglCnQNvng7b8QL3eIG215ftHeXQcFt+fHdZfHDvYZN6dAp6A6m+I7i5N6y0B6C5+YG2JPekS1CWrDrX/kMJjbhHwGiKB544jjw4QOJWJU0UdhnuLudWRzeoHZid3lRjhxOgUXHuz8ZJfHWJWnm5bqGGpVvIOelt4IdytvG7ZEn91TAMYlaSTBZmJsXk+/GZHJwwy7PFlfT71/E9f0TRhAtd0LiOEYAnX4D6KNtoI9y6HeRQnTNA3EMXuvxDkyYHehnBnWPc+kn5mDIo0yGz4KfTjQ3HBhMhlpFyCUauVSratLCZEIRdyocnjq+1baN8hmR1RJ/nq4RlyrljRqVEs7H4kSL3aX6yz+cajR+jjHkYSyu/876DbbwW2J484yGHDooOoHjuCnQcLcts4OH6gHzD3r86c3380KYDIzBxrh8hreQ1TvJOzim7StiSPJ1AOPHjz9w4ICvry+gJFTfWQ+HhnCcB6gKLR8StHxIUF0+tVoNF8UBVaG0fITB4jD1vBSE0vJRvOYCWj5EKH1zFG/4AF36EKHlQ4KWDwlaPiSoLh/ddbQduvQhQcuHBC0fEtBspuVrO3TpQ4KWDwlaPiRo+ZCgZ1yQoEsfEgwGw93d3u0mnQLVl4okEgmgMNSuGkwmrL+AwtDyIUHLhwQtHxK0fEhQ3XCh5Ws7dOlDgpYPCVo+JGj5kKDlQ4KWDwlaPiRo+ZCg5UOC+vJR8VRRamrqsWPHjDdmPNQFwXE8PT0dUAwqblpPSUkJCwvDDcBhL/wL5bP0oLXOhYryCYXCMWPGmIdA+ZKTkwH1oOiRidmzZ3fv3t30ViQSTZ48GVAPisoHF9gmTZpkOhAzbtw4Ly8qPkGaugd2Zs6caWzvgoKCpk6dCihJu/W8WVek5QWKJrnaFGI8xs1gYFqzM9Kmk+Xmp5Rb+dXRR8HAkpLi3Nz8oMCg6Oho49VIDogbTpCbzkWboowpH7nbeQybw/IRMgdOaJ/HebeDfJWFqqNflcIvzGRhKsXjg9uPVGh2mFv/FhCAwB8eAG92Lw9DHmbBoEyYRkNAowUzCwQtDogbzp3r/zHPq/cpRcALtP4UNg/XqnVaQtf7aa+hyag+b1DN5soi1Y87SvqN8u2VaJd7FYogLlL9erDUzQOPH4nUpKKWvi9W5k1/K5Ltmo8lObipcMg4vz7PtN2dBlLXcfjTcndfjotqBwmJEdw4VwMQQJKvXqz0F3GByxI3yFvRpAUIILV9av1DUIDrwuZjDx3itRUk+aCJQBBIv17nQrTq/B2FdvGJRNeWDzM9kqiNoMvnLL8UHYEOtfaiy0fpZzg5G1T5XLnstQOo8rl22evctg8afS5t93Vy2wfnNnQu/Uhi5LrT5Q0XNNAqLwZwV+470O8drfKaPXLUFUHv95BafvjrObv0wfn6kaMT0v/6E1AStNKna/lQ2q4GPeYFKKB2HZiDH19bW7Pjiy1Zt281NTUNHJj4j9nzQkIeLoc3NDZ8+eW2k78c9fT0Shgw+LV5i7p1e/zk881b3j9+4oivr9/wYaMWL3rbGFhcXLh12wf37t9hMJhhYRH/fPmNfvEJwH6Q7T6kto/QAYdWSrRa7bK33rh56/qypav37P7O28vnzQUvl5aVAMPR3XdWLRbXVG/ZvHPRwpVV1ZXvrF5s2l719d6dffr0h1HTX5x95Kfvz50/DQPr6moXLnpFKAzY9eWB7Z99Da+2fsNqUn+NzgOt63Cw9GVm3oTlZfWq9YMHPe3j45syf6mHp9fhwwdg1J9X0+7cyVqQshwWn9Gjxi9csCIyMgYWVWNGGDh2zLPwL5QPFsnMzAwY+MOh/WwOZ8Vba4ICRcHBoStXrFUo5EeP/QA6ELQxl4OlLzPrJovF6t9voPEthmHxfQfc+vsG0Lsovs/n80NDw4xRMdE916zeIBQ+9FLT+6l400U8PbyUSiV8kV+QGx3d03TcXCAQhAR3v3fvDrAbdMOlQ7sOqbRRrVZDQ8Q80MtLv+Avk0k5HIurTgyyI/m1NWKRKMQ8hMvjyRUOVF79KroLdR2w4efxeO9vaOaOkoHr/Rfy+QJY9cyfUG8TvkDQpGwyD1HI5cGiUGA3OsN/KCBVXp2DlRc2ZwqFAjb2sBUz/t+tW2BUVA8Y1bNHHOyL7z6qerCJXLr8dVijrVytR0wcbC5hcTa+hR13UXFBeHiH+rFEkw9Y9LxKyoD+gwYNevrjj9dXVlZIJPU/Hf1hfsqcU6eOAb0D4yGwJu7a9envaefhGAOaI9VVld27h1u52qRJL8AqDw0aeLXCwvyNH6zlcrjPPduh2wDRKi8Aju7x2Pj+1mM/H163YVV2dia0+MaMeXbqVL2vPNgDfPzhjo2b1q59byV8m5g4bOO/t1l/CkmwKOS9tR/s27d7xsyJ0FSMjX1q29bdsAMBHQjSHpcdK/Ii+rolPe+qXtwa67SHtxUs+iQKtBW0nhdz7cWOzjZcdF18oQ19utSl50uRQZ+wcuXi18lrHS4+Wd/Jax36bcp029dl6eSlova5hc6DAjMudOVFgd4ihETXLnz05lw0aPmQQJKPxcGZTAZwWVgYXOBEar2R5GPzmNI6Fz6Y8CBXykSTD2m2Oao3v7q0Q9dV25ec9DovYdud8wJE+ZKSfVlcxrHPS4ALcu2kRFqvnr5MBBBoh/O8Rz4rra/RBIW7+XfnEBa2DGGWTBzMcEK3VWJAlt7cs7QRvNUOuRYf1PpSTAyvF6uKcmRatXbuujCARvucJj97sLr4jlyt0qqUFuTDyNfk9EudACOax+m/cyuP2HpdWv0IrR1ntzx9bVyNNLPumSyMxcb9RLzJKQEAGao7154wYcL+/ftp59pthHZvjAQtHxIU9/ZElz4kKC0f7NagJcRgUHdcSHuLQYKWDwna1RMSdOlDgpYPCVo+JOi2Dwm69CFBy4cELR8StHxI0PIhQcuHBC0fErR8SNBmMxJ06UOClg8JqnuL8ff3BxSG0vJptdqqqipAYWhfRUjQ8iFBy4cELR8StHxI0PIhQXX5oO0CKAxd+pCg5UOC6vKZHhJETejShwQtHxK0fEjQ8iFBy4cELR8SVDxVtGjRorS0NNOTAXEcJwgCvr1+/TqgGFR0t7FkyZLg4GD8EcCgYGioA0/V7DCoKF9UVNTQoUPNqwUseiNGjADUg7rOtUNCHj8VF76eNm0aoB4UlU8kEo0ePdr4GjZ8CQkJRk/RVIO6roZmzJhh9O4O/7700kuAkrSn4dJQra0qUaiUhmf6tXD1bH7Mpmi2NgAABldJREFUW2f41YzvWhz3xh7F6rNwxiW+dr7pfO+YXooq/9vVDbrWp86JRw8AbXGEXEfymokDnIV7C9n+wWzQTqAaLrkZsr/O1NZWKbVaHYYZjA0MI8yckRvdYTf7SMeePaSz8qQn8ku1+sQW6eFNMlm4uzezR3/3hHFITsrbLt/578U56RKoGpvP5HtxfYM9eZ7t9qs6Fa2SqClpkNbIlTI10OmCInnJ84NAm2iLfLVF6u+2F8N83oGegbHt42K+s6gvlVfl12rUmv4jfYY85/B3cVi+s/urc65LfAI9gp6i6PMF2kB9uaLsTpWHD3P2KseMc8fkO3OwuiBTGjOMigMAdO5fKWHgurmpYfZncUC+n3aUlRU2xY3sDp5c7qWVMKGC68PsTG+vfCf3VBTfV/Qc/mSWO3MK08sxXPvyGrtKiV1mc0GWojBb1hW0g4QNDGySEb/srbQnsV3y/fqfcr8wL9Bl6DE8ND9Tak9K2/Kd2FOJYbgwsgvJB+F5cr9ZX2QzmW35inPkfpFPjo1iJxEDA6QSjaTaxhYRG/L9eaIOjnN8RXxASaSyuhX/Gnwz8yxwAmwe8/T+cutpbMh390YDx801hmLtjnegR025ynoaG/LJG7U+Ik/QJfEL99BodLUV1uqvtQmr+kodnHvyCnJWzW1orPn5l62FD/5WqZp6RA8ZM2Ku0F9vbZVX5m3+fObiN/acu/RN1p2Lnh7C+N5jnxu7wPg4oYy/T5/67UuFoiGu57ARSbOAM2Ey8awr9cOnWmz6rZW+/OxG4DS0Wu3OPW/mFd54YdI7by084Cbw+XTXXHFNCYxiMvQHsX44urFfn/EfvJc2c1rqxcv7b93WN3DllbkHDq1N6PfcO0sPJ8T/19ETm4EzwRi4uKzJSgJr8klr1QyGs6ajC4pvVokL/3taas+YRA9330kTFgv4Xr//8a0pQd9eo/o+NZrJZEWG9/f1FpWU5sDAK1cPe3kGjH3mVT7fIypiwOAEJ7sVwwm5zNpCs7XKq2qCRcRZ7osLi24xGKzoiIcOF+FEK5QpvzDDlCA4KNb0mst1VzTpq4K49kFAtwhTeIgoDjgXTKe1Nqi1Jh/OdOIiuqJJqtWqodlhHugmeDzjBm311rnk8gY/38crcGw2DzgTTIdZr3/W5PML4jjPJYK7my/88nNnNWu8bDqohHVWrX7cGCmVMuBMdFqCJ7B2ItaafLEDPC4dcdaRMlFgjEql8PLq5ufzcAWyprbUvPSR4u0VmJ3zu8kTaPbdNOBMYNslDOVaSWDt12bx9fVXXOCU/jc6cmDP6MQffnq/rr5CKqu/fPXQtp3/vHbjZ+u5+vYaA0caP53YDFuV3PzrV64eAs4E2m39x/pYSWBjodLDhy2plPqFuwMnMHf2lj/Sf/zP92uKHmT6+3Xv33fCsEQb67k9ogdPHL/oj2s/rlw7BHbBs15M3b77DSd5Dam8V8diM3hWW1cbncPfvzekHRPHjXqSZ5gtcS/tgVDEnvymtUU4G011n2EeOANU5kpA10Ol0FjXDtizywCuJd+9Xt8tinzkC1vxtRvHkkZpNCpo2ZE68A3wj1j4+leg/fi/fcsLim+RRqnVShaLxC0Cm8Vd+/YJYIG8q2W+AbadKdhl2X21uoDvzRc95Uca29AgJg1XqhQcC3YZg8EUCNpz/lUml2g15CdAFEoZj0PmtBfD4GiHPItEk/9XyYKPbTuatks+lQJ8tSa315hw0DW4c6Eofph34kTbq+Z2DWlhGeo/0j/7nO3J6yeA3Mv6amuPdsD+DWqJEz0HjPS+/VsheKK5c77IJ4Bhvw8Ux0a16Wcl6b+IIxNFHAGlH+7TNu5eKPYOYE1f5sA+TIcnBa6fq//juFjgzQtPaAd3IRShLLu2rrQhJEbw/HzHvlQb51T2phZKGzRu3rywAa4tYll2DRxWQds2+XVRQLjDbp/aPiV1P0N+6UglXAxhMHGugOXmx3cXCnjuVK/USrlWVqOQVsuVMqVKqWVysKcGeyUl+7TtasgzegQ4ubeiNF+hVGiNboHwVr6HUIBXwtpzzky/BRZnYBwOw0/EGTTeOzCCi3A1J5wqUkj1Ez02EllyhtU6XWtHWiSpDJez+UUYgMdltO8xNKq7eqI4tItPJGj5kKDlQ4KWDwlaPiRo+ZD4fwAAAP//MnI5bwAAAAZJREFUAwCTaDohNpNxmwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(echo_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure (ASCII):\")\n",
    "    print(echo_app.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "  [Human]: Hello, LangGraph!\n",
      "  [AI]: You said: Hello, LangGraph!\n"
     ]
    }
   ],
   "source": [
    "# Test the echo graph\n",
    "result = echo_app.invoke({\"messages\": [HumanMessage(content=\"Hello, LangGraph!\")]})\n",
    "\n",
    "print(\"Conversation:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"  [{role}]: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Task 3: Building a ReAct Agent from Scratch\n",
    "\n",
    "Now let's build something more sophisticated: a **ReAct agent** that can:\n",
    "1. **Reason** about what to do\n",
    "2. **Act** by calling tools\n",
    "3. **Observe** results\n",
    "4. **Repeat** until done\n",
    "\n",
    "This is exactly what `create_agent` does under the hood. Let's build it ourselves!\n",
    "\n",
    "### The Agent Loop Architecture\n",
    "\n",
    "```\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ    START     ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                           ‚ñº\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    agent     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "             ‚îÇ      ‚îÇ  (call LLM)  ‚îÇ         ‚îÇ\n",
    "             ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "             ‚îÇ             ‚îÇ                 ‚îÇ\n",
    "             ‚îÇ             ‚ñº                 ‚îÇ\n",
    "             ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n",
    "             ‚îÇ      ‚îÇ should_      ‚îÇ         ‚îÇ\n",
    "             ‚îÇ      ‚îÇ continue?    ‚îÇ         ‚îÇ\n",
    "             ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "             ‚îÇ             ‚îÇ                 ‚îÇ\n",
    "             ‚îÇ    tool_calls?                ‚îÇ\n",
    "             ‚îÇ     ‚îÇ           ‚îÇ             ‚îÇ\n",
    "             ‚îÇ    YES         NO             ‚îÇ\n",
    "             ‚îÇ     ‚îÇ           ‚îÇ             ‚îÇ\n",
    "             ‚îÇ     ‚ñº           ‚ñº             ‚îÇ\n",
    "             ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n",
    "             ‚îÇ ‚îÇ tools  ‚îÇ  ‚îÇ  END  ‚îÇ         ‚îÇ\n",
    "             ‚îî‚îÄ‚î§(execute‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "               ‚îÇ tools) ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [How to create a ReAct agent from scratch](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/)\n",
    "- [ReAct Agent Conceptual Guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#react-agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SinhaK\\AppData\\Local\\miniconda3\\envs\\agentenv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentState defined with messages field\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Step 1: Define the Agent State\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of our agent - just a list of messages.\"\"\"\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "print(\"AgentState defined with messages field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "# # Step 2: Initialize our local LLM with Ollama\n",
    "# llm = ChatOllama(\n",
    "#     model=\"gpt-oss:20b\",\n",
    "#     temperature=0,  # Deterministic for reproducibility\n",
    "# )\n",
    "\n",
    "# print(f\"LLM initialized: {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feeb45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see LCEL in action with a simple example\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create our components (each is a Runnable)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that speaks like a pirate.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5\", temperature=1)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together with LCEL\n",
    "pirate_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Task 4: Adding Tools to Your Agent\n",
    "\n",
    "Tools are functions that the agent can call. We use the `@tool` decorator and **bind** them to the LLM.\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [LangChain Tools Conceptual Guide](https://python.langchain.com/docs/concepts/tools/)\n",
    "- [@tool Decorator Reference](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)\n",
    "- [ToolNode Prebuilt](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools defined and bound to LLM:\n",
      "  - calculate: Evaluate a mathematical expression. Use this for a...\n",
      "  - get_current_time: Get the current date and time. Use this when the u...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Tools\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression. Use this for any math calculations.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression to evaluate (e.g., '2 + 2', '10 * 5')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using eval with restricted globals for safety\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when the user asks about the current time or date.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"The current date and time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Create our tool list\n",
    "tools = [calculate, get_current_time]\n",
    "\n",
    "# Bind tools to the LLM - this tells the LLM about available tools\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Tools defined and bound to LLM:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent node defined\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the Agent Node (calls the LLM)\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that can perform calculations and tell the time.\n",
    "Always use the available tools when appropriate.\n",
    "Be concise in your responses.\"\"\"\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"The agent node - calls the LLM with the current conversation.\"\"\"\n",
    "    # Prepare messages with system prompt\n",
    "    messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    \n",
    "    # Call the LLM\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return the response to be added to state\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"Agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool node created using ToolNode prebuilt\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Define the Tool Node (executes tools)\n",
    "# We can use LangGraph's prebuilt ToolNode for convenience\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"Tool node created using ToolNode prebuilt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional routing function defined\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the Conditional Edge (routing logic)\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Determine whether to call tools or end the conversation.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # If the LLM made tool calls, route to tools node\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, end the conversation\n",
    "    return \"end\"\n",
    "\n",
    "print(\"Conditional routing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct agent built from scratch!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Build the Graph!\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edge from agent\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # If should_continue returns \"tools\", go to tools node\n",
    "        \"end\": END         # If should_continue returns \"end\", finish\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edge from tools back to agent (the loop!)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "print(\"ReAct agent built from scratch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize our agent\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure (ASCII):\")\n",
    "    print(agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing our from-scratch agent:\n",
      "==================================================\n",
      "\n",
      "Conversation:\n",
      "  [HumanMessage]: What is 25 * 48?\n",
      "  [AIMessage]: [Tool calls: [{'name': 'calculate', 'args': {'expression': '25 * 48'}, 'id': 'call_20NNCXEQDqhHFMyRDPl48B5U', 'type': 'tool_call'}]]\n",
      "  [ToolMessage]: The result of 25 * 48 is 1200\n",
      "  [AIMessage]: 1200\n"
     ]
    }
   ],
   "source": [
    "# Test our agent!\n",
    "print(\"Testing our from-scratch agent:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is 25 * 48?\")]})\n",
    "\n",
    "print(\"\\nConversation:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    msg_type = type(msg).__name__\n",
    "    content = msg.content if msg.content else f\"[Tool calls: {msg.tool_calls}]\" if hasattr(msg, 'tool_calls') and msg.tool_calls else \"[No content]\"\n",
    "    print(f\"  [{msg_type}]: {content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with multiple tool calls:\n",
      "==================================================\n",
      "\n",
      "Final response:\n",
      "It‚Äôs 22:24:47 on 2026-01-25. 100 divided by the current hour (22) is approximately 4.545454545454546.\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple tools\n",
    "print(\"Testing with multiple tool calls:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What time is it, and what is 100 divided by the current hour?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming agent execution:\n",
      "==================================================\n",
      "\n",
      "[Node: agent]\n",
      "  Tool calls: ['calculate']\n",
      "\n",
      "[Node: tools]\n",
      "  Content: The result of 200 * 0.15 is 30.0\n",
      "\n",
      "[Node: agent]\n",
      "  Content: 30\n"
     ]
    }
   ],
   "source": [
    "# Stream the agent's execution to see it step by step\n",
    "print(\"Streaming agent execution:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Calculate 15% of 200\")]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    for node_name, values in chunk.items():\n",
    "        print(f\"\\n[Node: {node_name}]\")\n",
    "        if \"messages\" in values:\n",
    "            for msg in values[\"messages\"]:\n",
    "                if hasattr(msg, 'content') and msg.content:\n",
    "                    print(f\"  Content: {msg.content[:200]}\")\n",
    "                if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                    print(f\"  Tool calls: {[tc['name'] for tc in msg.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ùì Question #1:\n",
    "\n",
    "In our from-scratch agent, we defined a `should_continue` function that returns either `\"tools\"` or `\"end\"`. How does this compare to how `create_agent` handles the same decision? What additional logic might `create_agent` include that we didn't implement?\n",
    "\n",
    "##### Answer:\n",
    "**should_continue** does one simple check:\n",
    "\n",
    "Look at the last LLM message ‚Üí if it contains tool_calls ‚Üí go to \"tools\" else ‚Üí \"end\"\n",
    "\n",
    "So your loop decision is basically:\n",
    "\n",
    "LLM output has tool_calls? YES ‚Üí tools NO ‚Üí end\n",
    "\n",
    "That‚Äôs the same high-level decision rule used in most agent implementations.\n",
    "\n",
    "How **create_agent** handles the same decision\n",
    "\n",
    "**create_agent** does the same ‚Äútool_calls vs final‚Äù routing, but usually with additional logic around it. Common extras include:\n",
    "\n",
    "***1. More robust detection***\n",
    "\n",
    "Ensures the last message is the right type (AI/assistant message)\n",
    "\n",
    "Handles cases like empty tool_calls, malformed tool_calls, or unexpected response formats\n",
    "\n",
    "---\n",
    "\n",
    "***2. Tool call validation / normalization***\n",
    "\n",
    "Checks tool name exists\n",
    "\n",
    "Checks required args are present and match schema\n",
    "\n",
    "May coerce/normalize arguments (depending on stack)\n",
    "\n",
    "---\n",
    "\n",
    "***3. Multiple tool calls in one turn***\n",
    "\n",
    "If the model calls 2‚Äì3 tools at once, create_agent handles executing them (possibly in sequence or batch) and appending multiple ToolMessages correctly.\n",
    "\n",
    "---\n",
    "\n",
    "***4. Stop conditions and safety limits***\n",
    "\n",
    "Max iterations / max model calls (to prevent infinite loops)\n",
    "\n",
    "Early stopping behavior (e.g., ‚Äúif stuck, produce best-effort answer‚Äù)\n",
    "\n",
    "---\n",
    "\n",
    "***5. Error routing***\n",
    "\n",
    "If a tool fails, the agent may return a tool error message back into the loop so the model can recover (instead of crashing the graph).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "****In short: your should_continue is the minimal gate. create_agent typically adds correctness + safety + ‚Äúreal-world messiness handling.‚Äù****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## ‚ùì Question #2:\n",
    "\n",
    "We used `ToolNode` from `langgraph.prebuilt` to execute tools. Looking at the tool execution flow, what would happen if we wanted to add logging, error handling, or rate limiting to tool execution? How would building our own tool node give us more control?\n",
    "\n",
    "##### Answer:\n",
    "*Right now you‚Äôre using:\n",
    "\n",
    "***tool_node = ToolNode(tools)***\n",
    "\n",
    "ToolNode is convenient, but it‚Äôs a black box: it runs tool calls and returns tool outputs. If you want production behaviors like:\n",
    "\n",
    "1. per-tool logging (inputs/outputs/latency)\n",
    "\n",
    "2. try/except error handling + retries\n",
    "\n",
    "3. rate limits (per user, per tool, per minute)\n",
    "\n",
    "4. timeouts\n",
    "\n",
    "5. redaction of sensitive values\n",
    "\n",
    "6. fallback logic (‚Äúif KB search fails, try a different retriever‚Äù)\n",
    "\n",
    "\n",
    "‚Ä¶you either have to wrap each tool individually or replace ToolNode with your own node.\n",
    "\n",
    "***How building your own tool node gives more control***\n",
    "\n",
    "If you build your own tool node, you control the full execution loop:\n",
    "\n",
    "Last LLM msg ‚Üí read tool_calls ‚Üí for each call:\n",
    "\n",
    "1. log start time + args\n",
    "\n",
    "2. apply rate limit / quota check\n",
    "\n",
    "3. run tool with timeout\n",
    "\n",
    "4. catch exceptions\n",
    "\n",
    "5. optionally retry/backoff\n",
    "\n",
    "6. log result/error\n",
    "\n",
    "7. return a ToolMessage (success) or ToolMessage (error) back to state\n",
    "\n",
    "Therefore we can do al of the below:\n",
    "\n",
    "1. Before tool call: validate, rate limit, log\n",
    "\n",
    "2. After tool call: log output, sanitize, cache\n",
    "\n",
    "On failure: retry/backoff or return structured error to the model\n",
    "\n",
    "\n",
    "***Concrete example of what your custom node would enable***\n",
    "\n",
    "You could enforce rules like:\n",
    "\n",
    "1. ‚ÄúKB search tool max 3 calls per user query‚Äù\n",
    "\n",
    "2. ‚ÄúNo tool may run longer than 5 seconds‚Äù*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## üèóÔ∏è Activity #1: Implement a Custom Routing Function\n",
    "\n",
    "Extend the agent by implementing a **custom routing function** that adds more sophisticated logic.\n",
    "\n",
    "Ideas:\n",
    "- Add a maximum iteration limit to prevent infinite loops\n",
    "- Route to different nodes based on the type of tool being called\n",
    "- Add a \"thinking\" step before tool execution\n",
    "\n",
    "Requirements:\n",
    "1. Modify the `should_continue` function or create a new one\n",
    "2. Add any new nodes if needed\n",
    "3. Rebuild and test the agent\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges)\n",
    "- [How to create branches for parallel node execution](https://langchain-ai.github.io/langgraph/how-tos/branching/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c9707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph compiled. Tool groups: {'retrieval': ['search_wellness_knowledge'], 'utility': ['calculate', 'calculate_bmi', 'create_workout_plan', 'estimate_daily_calories', 'get_current_time']}\n",
      "\n",
      "--- TEST 1: Utility tools (math + time) ---\n",
      "[THINK] Plan: call tool(s) ['calculate', 'get_current_time'], then answer.\n",
      "[TOOL START] calculate args={'expression': '19*23'}\n",
      "[TOOL OK] calculate (1.0 ms)\n",
      "[TOOL START] get_current_time args={'tz': 'UTC'}\n",
      "[TOOL OK] get_current_time (0.6 ms)\n",
      "The result of 19 multiplied by 23 is 437. The current time in UTC is 05:51:33 on January 26, 2026.\n",
      "\n",
      "--- TEST 2: Retrieval tool (KB) ---\n",
      "[THINK] Plan: call tool(s) ['search_wellness_knowledge'], then answer.\n",
      "[TOOL START] search_wellness_knowledge args={'query': 'progressive overload', 'top_k': 3}\n",
      "[TOOL OK] search_wellness_knowledge (0.0 ms)\n",
      "Progressive overload is the practice of adding a small load (2‚Äì5%) or 1 repetition weekly while keeping 1‚Äì3 repetitions in reserve (RIR). The load is increased after reaching the top of the rep range. This method helps in gradually increasing the intensity of workouts to promote strength and muscle gains. (Source: KB2)\n",
      "\n",
      "--- TEST 3: BMI tool ---\n",
      "[THINK] Plan: call tool(s) ['calculate_bmi'], then answer.\n",
      "[TOOL START] calculate_bmi args={'height_cm': 180, 'weight_kg': 75}\n",
      "[TOOL OK] calculate_bmi (0.0 ms)\n",
      "Your BMI is approximately 23.15, which falls into the \"Normal weight\" category. If you have any other questions or need further assistance, feel free to ask!\n",
      "\n",
      "--- TEST 4: Calories tool ---\n",
      "[THINK] Plan: call tool(s) ['estimate_daily_calories', 'estimate_daily_calories'], then answer.\n",
      "[TOOL START] estimate_daily_calories args={'sex': 'male', 'age_years': 30, 'height_cm': 180, 'weight_kg': 75, 'activity_level': 'moderate', 'goal': 'maintain'}\n",
      "[TOOL OK] estimate_daily_calories (0.0 ms)\n",
      "[TOOL START] estimate_daily_calories args={'sex': 'male', 'age_years': 30, 'height_cm': 180, 'weight_kg': 75, 'activity_level': 'moderate', 'goal': 'lose'}\n",
      "[TOOL OK] estimate_daily_calories (0.0 ms)\n",
      "Your estimated maintenance calories are about 2682 kcal per day. For fat loss, a target of around 2182 kcal per day is recommended. This creates a calorie deficit to help you lose fat while maintaining your activity level.\n",
      "\n",
      "--- TEST 5: Workout plan tool ---\n",
      "[THINK] Plan: call tool(s) ['create_workout_plan'], then answer.\n",
      "[TOOL START] create_workout_plan args={'goal': 'muscle_gain', 'days_per_week': 4, 'equipment': 'dumbbells', 'experience': 'beginner', 'session_minutes': 45}\n",
      "[TOOL OK] create_workout_plan (0.0 ms)\n",
      "Here is a 4-day per week muscle gain workout plan using dumbbells for a beginner, with each session lasting about 45 minutes:\n",
      "\n",
      "Day 1:\n",
      "- Warm-up: 5‚Äì8 minutes easy cardio + dynamic warm-up\n",
      "- Work:\n",
      "  - Dumbbell (DB) press: 3 sets of 6‚Äì12 reps\n",
      "  - DB row: 3 sets of 6‚Äì12 reps\n",
      "  - Goblet squat: 3 sets of 6‚Äì12 reps\n",
      "  - DB Romanian deadlift (RDL): 3 sets of 6‚Äì12 reps\n",
      "  - Side plank: 3 sets of 6‚Äì12 reps\n",
      "- Cooldown: 3‚Äì5 minutes easy walk + light stretching\n",
      "\n",
      "Day 2:\n",
      "- Warm-up: 5‚Äì8 minutes easy cardio + dynamic warm-up\n",
      "- Work:\n",
      "  - DB press: 3 sets of 6‚Äì12 reps\n",
      "  - DB row: 3 sets of 6‚Äì12 reps\n",
      "  - Goblet squat: 3 sets of 6‚Äì12 reps\n",
      "  - DB RDL: 3 sets of 6‚Äì12 reps\n",
      "  - Side plank: 3 sets of 6‚Äì12 reps\n",
      "- Cooldown: 3‚Äì5 minutes easy walk + light stretching\n",
      "\n",
      "Day 3:\n",
      "- Warm-up: 5‚Äì8 minutes easy cardio + dynamic warm-up\n",
      "- Work:\n",
      "  - DB press: 3 sets of 6‚Äì12 reps\n",
      "  - DB row: 3 sets of 6‚Äì12 reps\n",
      "  - Goblet squat: 3 sets of 6‚Äì12 reps\n",
      "  - DB RDL: 3 sets of 6‚Äì12 reps\n",
      "  - Side plank: 3 sets of 6‚Äì12 reps\n",
      "- Cooldown: 3‚Äì5 minutes easy walk + light stretching\n",
      "\n",
      "Day 4:\n",
      "- Warm-up: 5‚Äì8 minutes easy cardio + dynamic warm-up\n",
      "- Work:\n",
      "  - DB press: 3 sets of 6‚Äì12 reps\n",
      "  - DB row: 3 sets of 6‚Äì12 reps\n",
      "  - Goblet squat: 3 sets of 6‚Äì12 reps\n",
      "  - DB RDL: 3 sets of 6‚Äì12 reps\n",
      "  - Side plank: 3 sets of 6‚Äì12 reps\n",
      "- Cooldown: 3‚Äì5 minutes easy walk + light stretching\n",
      "\n",
      "This plan focuses on compound movements with dumbbells to build muscle effectively while allowing for recovery between sessions.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports (messages, tools, graph, LLM wrapper) ---\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Literal\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Define Tools (DON'T assume they exist: we define them)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Tool #1: calculator\n",
    "class CalculateArgs(BaseModel):\n",
    "    expression: str = Field(..., description=\"A simple math expression, e.g. '19*23'.\")\n",
    "\n",
    "@tool(\"calculate\", args_schema=CalculateArgs)\n",
    "def calculate(expression: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a simple math expression.\n",
    "    Use this when the user asks for arithmetic or numeric calculations.\n",
    "    Input: expression (string)\n",
    "    Output: result (number as string)\n",
    "    \"\"\"\n",
    "    # Very simple evaluator: allow only safe characters (digits/operators/parentheses/decimal/space)\n",
    "    allowed = set(\"0123456789+-*/(). %\")\n",
    "    if any(ch not in allowed for ch in expression):\n",
    "        raise ValueError(\"Expression contains unsupported characters.\")\n",
    "    # eval is used here only for demonstration; in production you'd use a safe math parser.\n",
    "    result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "    return {\"expression\": expression, \"result\": result}\n",
    "\n",
    "# Tool #2: current time\n",
    "class TimeArgs(BaseModel):\n",
    "    tz: str = Field(\"America/Chicago\", description=\"IANA timezone string, e.g. 'UTC' or 'America/Chicago'.\")\n",
    "\n",
    "@tool(\"get_current_time\", args_schema=TimeArgs)\n",
    "def get_current_time(tz: str = \"America/Chicago\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get the current time in a given timezone.\n",
    "    Use this when the user asks for the current time 'now' in a city/timezone.\n",
    "    \"\"\"\n",
    "    now = datetime.now(ZoneInfo(tz))\n",
    "    return {\"timezone\": tz, \"iso\": now.isoformat(), \"readable\": now.strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "# Tool #3: simple \"wellness KB search\" (toy KB for demo)\n",
    "# In your real notebook, this would query your vector DB / docs store.\n",
    "class KBArgs(BaseModel):\n",
    "    query: str = Field(..., description=\"User question to search inside the wellness knowledge base.\")\n",
    "    top_k: int = Field(3, description=\"How many results to return.\")\n",
    "\n",
    "@tool(\"search_wellness_knowledge\", args_schema=KBArgs)\n",
    "def search_wellness_knowledge(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search a wellness knowledge base and return evidence snippets with citations.\n",
    "    Use this when you need factual wellness guidance (progressive overload, deload, recovery).\n",
    "    \"\"\"\n",
    "    # Tiny fake KB entries (replace with your real KB retrieval)\n",
    "    kb = [\n",
    "        {\"id\": \"KB1\", \"title\": \"Deload Guidance\", \"text\": \"If fatigue stalls performance for ~2+ weeks, reduce volume by ~30‚Äì50% for 1 week (deload).\"},\n",
    "        {\"id\": \"KB2\", \"title\": \"Progressive Overload\", \"text\": \"Add small load (2‚Äì5%) or 1 rep weekly while keeping 1‚Äì3 reps in reserve (RIR). Increase load after topping rep range.\"},\n",
    "        {\"id\": \"KB3\", \"title\": \"Recovery Basics\", \"text\": \"Sleep and hydration strongly influence recovery, performance, and muscle gain.\"},\n",
    "    ]\n",
    "    # Naive retrieval: keyword match score\n",
    "    q = query.lower()\n",
    "    scored = []\n",
    "    for item in kb:\n",
    "        score = sum(1 for w in q.split() if w in item[\"text\"].lower() or w in item[\"title\"].lower())\n",
    "        scored.append((score, item))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    hits = [item for score, item in scored[:top_k] if score > 0] or scored[:1] # return at least 1\n",
    "    return {\"query\": query, \"results\": hits}\n",
    "\n",
    "# Tool #4: BMI\n",
    "class BMISpec(BaseModel):\n",
    "    height_cm: float = Field(..., gt=0, description=\"Height in centimeters.\")\n",
    "    weight_kg: float = Field(..., gt=0, description=\"Weight in kilograms.\")\n",
    "\n",
    "@tool(\"calculate_bmi\", args_schema=BMISpec)\n",
    "def calculate_bmi(height_cm: float, weight_kg: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate BMI (Body Mass Index) and category.\n",
    "    Use when user asks for BMI / weight category based on height & weight.\n",
    "    \"\"\"\n",
    "    h_m = height_cm / 100.0\n",
    "    bmi = weight_kg / (h_m ** 2)\n",
    "    if bmi < 18.5:\n",
    "        cat = \"Underweight\"\n",
    "    elif bmi < 25:\n",
    "        cat = \"Normal weight\"\n",
    "    elif bmi < 30:\n",
    "        cat = \"Overweight\"\n",
    "    else:\n",
    "        cat = \"Obesity\"\n",
    "    return {\"bmi\": round(bmi, 2), \"category\": cat}\n",
    "\n",
    "# Tool #5: Calorie estimate (Mifflin-St Jeor)\n",
    "class CalorieSpec(BaseModel):\n",
    "    sex: Literal[\"male\", \"female\"] = Field(..., description=\"Biological sex (for BMR equation).\")\n",
    "    age_years: int = Field(..., ge=13, le=120, description=\"Age in years.\")\n",
    "    height_cm: float = Field(..., gt=0, description=\"Height in centimeters.\")\n",
    "    weight_kg: float = Field(..., gt=0, description=\"Weight in kilograms.\")\n",
    "    activity_level: Literal[\"sedentary\",\"light\",\"moderate\",\"active\",\"very_active\"] = Field(..., description=\"Activity level.\")\n",
    "    goal: Literal[\"maintain\",\"lose\",\"gain\"] = Field(\"maintain\", description=\"Goal to adjust calories.\")\n",
    "\n",
    "_ACTIVITY = {\"sedentary\":1.2, \"light\":1.375, \"moderate\":1.55, \"active\":1.725, \"very_active\":1.9}\n",
    "\n",
    "@tool(\"estimate_daily_calories\", args_schema=CalorieSpec)\n",
    "def estimate_daily_calories(sex: str, age_years: int, height_cm: float, weight_kg: float,\n",
    "                            activity_level: str, goal: str = \"maintain\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Estimate daily calories (TDEE) using Mifflin-St Jeor + activity multiplier.\n",
    "    Use when user asks for maintenance calories or a cut/bulk target.\n",
    "    \"\"\"\n",
    "    if sex == \"male\":\n",
    "        bmr = 10*weight_kg + 6.25*height_cm - 5*age_years + 5\n",
    "    else:\n",
    "        bmr = 10*weight_kg + 6.25*height_cm - 5*age_years - 161\n",
    "    tdee = bmr * _ACTIVITY[activity_level]\n",
    "    if goal == \"lose\":\n",
    "        target = tdee - 500\n",
    "    elif goal == \"gain\":\n",
    "        target = tdee + 300\n",
    "    else:\n",
    "        target = tdee\n",
    "    return {\"bmr\": int(round(bmr)), \"maintenance\": int(round(tdee)), \"target\": int(round(target)), \"goal\": goal}\n",
    "\n",
    "# Tool #6: Workout plan\n",
    "class WorkoutPlanSpec(BaseModel):\n",
    "    goal: Literal[\"fat_loss\",\"muscle_gain\",\"general_fitness\"] = Field(..., description=\"Fitness goal.\")\n",
    "    days_per_week: int = Field(..., ge=2, le=6, description=\"Days per week (2-6).\")\n",
    "    equipment: Literal[\"none\",\"dumbbells\",\"full_gym\"] = Field(..., description=\"Available equipment.\")\n",
    "    experience: Literal[\"beginner\",\"intermediate\"] = Field(..., description=\"Experience level.\")\n",
    "    session_minutes: int = Field(..., ge=15, le=90, description=\"Minutes per session.\")\n",
    "    constraints: Optional[str] = Field(None, description=\"Constraints/injuries/preferences.\")\n",
    "\n",
    "@tool(\"create_workout_plan\", args_schema=WorkoutPlanSpec)\n",
    "def create_workout_plan(goal: str, days_per_week: int, equipment: str, experience: str,\n",
    "                        session_minutes: int, constraints: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a simple weekly workout plan.\n",
    "    Use when user asks for a workout routine and provides goal/schedule/equipment.\n",
    "    \"\"\"\n",
    "    warmup = \"5‚Äì8 min easy cardio + dynamic warm-up\"\n",
    "    cooldown = \"3‚Äì5 min easy walk + light stretching\"\n",
    "    base = {\n",
    "        \"none\": [\"push-ups\", \"bodyweight squats\", \"glute bridges\", \"plank\"],\n",
    "        \"dumbbells\": [\"DB press\", \"DB row\", \"goblet squat\", \"DB RDL\", \"side plank\"],\n",
    "        \"full_gym\": [\"bench press\", \"lat pulldown\", \"squat/leg press\", \"RDL\", \"cable crunch\"],\n",
    "    }[equipment]\n",
    "    sets = 3 if experience == \"beginner\" else 4\n",
    "    reps = \"8‚Äì12\" if goal != \"muscle_gain\" else \"6‚Äì12\"\n",
    "    plan_days = []\n",
    "    for d in range(days_per_week):\n",
    "        plan_days.append({\n",
    "            \"day\": f\"Day {d+1}\",\n",
    "            \"warmup\": warmup,\n",
    "            \"work\": [f\"{ex}: {sets} x {reps}\" for ex in base],\n",
    "            \"cooldown\": cooldown,\n",
    "        })\n",
    "    return {\"goal\": goal, \"constraints\": constraints, \"session_minutes\": session_minutes, \"plan\": plan_days}\n",
    "\n",
    "# Collect all tools into a registry list\n",
    "TOOLS = [\n",
    "    search_wellness_knowledge,\n",
    "    calculate,\n",
    "    get_current_time,\n",
    "    calculate_bmi,\n",
    "    estimate_daily_calories,\n",
    "    create_workout_plan,\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Define tool groups for routing (retrieval vs utility)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "RETRIEVAL_TOOLS = {\"search_wellness_knowledge\"} # \"RAG tool\"\n",
    "UTILITY_TOOLS = {t.name for t in TOOLS if t.name not in RETRIEVAL_TOOLS} # everything else\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Define Agent State (messages + iteration counters + tool budgets)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    iterations: int\n",
    "    tool_calls_used: Dict[str, int]\n",
    "    last_plan: Optional[str] # <-- add this\n",
    "\n",
    "MAX_ITERS = 6 # max model calls to prevent infinite loops\n",
    "MAX_TOOL_CALLS_PER_TOOL = 3 # simple per-tool cap (demo rate limiting)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) LLM setup (bind tools so the model can emit tool_calls)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# We use ChatOpenAI as the wrapper (still uses OpenAI under the hood).\n",
    "# Pick a tool-calling capable model you have access to.\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0).bind_tools(TOOLS)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) Node A: Model call node (increments iterations + calls the LLM)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    # Increase iteration count each time the model is called\n",
    "    state[\"iterations\"] = state.get(\"iterations\", 0) + 1\n",
    "\n",
    "    # Call the model with the current conversation messages\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # Append model response to the message list\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Node B: \"Thinking\" step before tool execution (short plan, no chain-of-thought)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def thinking_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    'Thinking' step that does NOT add a message.\n",
    "    We store the plan in state and optionally print it.\n",
    "    This avoids violating OpenAI's tool-calling message ordering.\n",
    "    \"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last, \"tool_calls\", None) or []\n",
    "\n",
    "    if not tool_calls:\n",
    "        state[\"last_plan\"] = None\n",
    "        return state\n",
    "\n",
    "    tool_names = [tc.get(\"name\") for tc in tool_calls if tc.get(\"name\")]\n",
    "    plan = f\"Plan: call tool(s) {tool_names}, then answer.\"\n",
    "\n",
    "    state[\"last_plan\"] = plan\n",
    "    print(\"[THINK]\", plan) # optional debug output\n",
    "    return state\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Node C: Custom tool execution node (logging + error handling + rate limiting)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Build a name->tool mapping so we can execute tool calls by tool name\n",
    "TOOL_BY_NAME = {t.name: t for t in TOOLS}\n",
    "\n",
    "def run_tools(state: AgentState) -> AgentState:\n",
    "    # Find the most recent model message that contains tool_calls\n",
    "    tool_calls = []\n",
    "    tool_call_source = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        tc = getattr(msg, \"tool_calls\", None) or []\n",
    "        if tc:\n",
    "            tool_calls = tc\n",
    "            tool_call_source = msg\n",
    "            break\n",
    "\n",
    "    # If none found, there's nothing to execute\n",
    "    if not tool_calls:\n",
    "        return state\n",
    "\n",
    "    # Ensure we have a per-tool counter dictionary\n",
    "    state[\"tool_calls_used\"] = state.get(\"tool_calls_used\", {})\n",
    "\n",
    "    # Execute each tool call in order\n",
    "    for tc in tool_calls:\n",
    "        tool_name = tc.get(\"name\")\n",
    "        tool_args = tc.get(\"args\", {}) or {}\n",
    "        tool_id = tc.get(\"id\") # tool call id is required for ToolMessage linkage\n",
    "\n",
    "        # -------- Logging (before tool) --------\n",
    "        t0 = time.time()\n",
    "        print(f\"[TOOL START] {tool_name} args={tool_args}\")\n",
    "\n",
    "        # -------- Rate limiting --------\n",
    "        used = state[\"tool_calls_used\"].get(tool_name, 0)\n",
    "        if used >= MAX_TOOL_CALLS_PER_TOOL:\n",
    "            err = f\"Rate limit: tool '{tool_name}' exceeded {MAX_TOOL_CALLS_PER_TOOL} calls.\"\n",
    "            print(f\"[TOOL BLOCKED] {err}\")\n",
    "            state[\"messages\"] = state[\"messages\"] + [ToolMessage(content=err, tool_call_id=tool_id)]\n",
    "            continue\n",
    "        state[\"tool_calls_used\"][tool_name] = used + 1\n",
    "\n",
    "        # -------- Execute tool with error handling --------\n",
    "        try:\n",
    "            if tool_name not in TOOL_BY_NAME:\n",
    "                raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "            tool_obj = TOOL_BY_NAME[tool_name]\n",
    "\n",
    "            # .invoke expects a dict of args for LangChain tools\n",
    "            output = tool_obj.invoke(tool_args)\n",
    "\n",
    "            # Convert tool output to a string for ToolMessage (safe + readable)\n",
    "            content = str(output)\n",
    "\n",
    "            # Append ToolMessage so the LLM can see the tool result\n",
    "            state[\"messages\"] = state[\"messages\"] + [ToolMessage(content=content, tool_call_id=tool_id)]\n",
    "\n",
    "            dt = (time.time() - t0) * 1000\n",
    "            print(f\"[TOOL OK] {tool_name} ({dt:.1f} ms)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Append error as ToolMessage so the model can recover and try a different approach\n",
    "            dt = (time.time() - t0) * 1000\n",
    "            err = f\"Tool '{tool_name}' failed: {type(e).__name__}: {e}\"\n",
    "            print(f\"[TOOL ERROR] {tool_name} ({dt:.1f} ms) -> {err}\")\n",
    "            state[\"messages\"] = state[\"messages\"] + [ToolMessage(content=err, tool_call_id=tool_id)]\n",
    "\n",
    "    return state\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) Custom routing function (max iters + route by tool type + thinking step)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def route_next(state: AgentState) -> Literal[\"end\", \"think_then_tools\"]:\n",
    "    # Stop if max iterations reached\n",
    "    if state.get(\"iterations\", 0) >= MAX_ITERS:\n",
    "        return \"end\"\n",
    "\n",
    "    # Check if last model message has tool_calls\n",
    "    last = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last, \"tool_calls\", None) or []\n",
    "\n",
    "    # If no tools requested -> end\n",
    "    if not tool_calls:\n",
    "        return \"end\"\n",
    "\n",
    "    # Otherwise -> go to thinking, then tool execution\n",
    "    return \"think_then_tools\"\n",
    "\n",
    "# Optional: classify tool type (retrieval vs utility) for debugging/telemetry\n",
    "def classify_last_tool_type(state: AgentState) -> str:\n",
    "    last = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last, \"tool_calls\", None) or []\n",
    "    if not tool_calls:\n",
    "        return \"none\"\n",
    "    tool_name = tool_calls[0].get(\"name\")\n",
    "    return \"retrieval\" if tool_name in RETRIEVAL_TOOLS else \"utility\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9) Build LangGraph (agent -> route -> thinking -> tools -> agent loop)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"agent\", call_model) # model call node\n",
    "graph_builder.add_node(\"thinking\", thinking_step) # planning node\n",
    "graph_builder.add_node(\"tools\", run_tools) # custom tool node\n",
    "\n",
    "# Conditional routing after the agent runs\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    route_next,\n",
    "    {\n",
    "        \"think_then_tools\": \"thinking\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# After thinking, always run tools (because thinking only runs when tool_calls exist)\n",
    "graph_builder.add_edge(\"thinking\", \"tools\")\n",
    "\n",
    "# After tools, loop back to the agent so it can read tool outputs and decide again\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Set entrypoint and compile\n",
    "graph_builder.set_entry_point(\"agent\")\n",
    "agent_graph = graph_builder.compile()\n",
    "\n",
    "print(\"‚úÖ Graph compiled. Tool groups:\",\n",
    "      {\"retrieval\": list(RETRIEVAL_TOOLS), \"utility\": sorted(list(UTILITY_TOOLS))})\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 10) Helper: run the agent on a prompt\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def run_agent(prompt: str) -> str:\n",
    "    # Create initial state for a new conversation\n",
    "    state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=prompt)], # start with the user's message\n",
    "        \"iterations\": 0, # zero model calls so far\n",
    "        \"tool_calls_used\": {}, # tool usage counts\n",
    "    }\n",
    "\n",
    "    # Invoke the graph\n",
    "    final_state = agent_graph.invoke(state)\n",
    "\n",
    "    # Return the last assistant message content (final answer)\n",
    "    # Scan backwards to find the last AIMessage with non-empty content\n",
    "    for msg in reversed(final_state[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage) and msg.content:\n",
    "            return msg.content\n",
    "    return \"(No final assistant message found.)\"\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 11) Quick tests (edit these prompts to test your tool routing)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- TEST 1: Utility tools (math + time) ---\")\n",
    "print(run_agent(\"What is 19*23 and what time is it in UTC?\"))\n",
    "\n",
    "print(\"\\n--- TEST 2: Retrieval tool (KB) ---\")\n",
    "print(run_agent(\"According to the wellness knowledge base, what is progressive overload? Include citations.\"))\n",
    "\n",
    "print(\"\\n--- TEST 3: BMI tool ---\")\n",
    "print(run_agent(\"My height is 180 cm and weight is 75 kg. What's my BMI and category?\"))\n",
    "\n",
    "print(\"\\n--- TEST 4: Calories tool ---\")\n",
    "print(run_agent(\"I'm male, 30 years old, 180 cm, 75 kg, moderate activity. Give my maintenance calories and a fat loss target.\"))\n",
    "\n",
    "print(\"\\n--- TEST 5: Workout plan tool ---\")\n",
    "print(run_agent(\"Create a workout plan: goal=muscle_gain, days_per_week=4, equipment=dumbbells, experience=beginner, session_minutes=45.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "# Breakout Room #2\n",
    "## Agentic RAG with Local Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "Now let's build a full **Agentic RAG** system from scratch using our local models!\n",
    "\n",
    "We'll transition from the `aimakerspace` utilities to the **LangChain ecosystem**:\n",
    "\n",
    "| Task | aimakerspace | LangChain |\n",
    "|------|--------------|----------|\n",
    "| Load Documents | `TextFileLoader` | `TextLoader` |\n",
    "| Split Text | `CharacterTextSplitter` | `RecursiveCharacterTextSplitter` |\n",
    "| Embeddings | Custom | `OllamaEmbeddings` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Task 5: Loading & Chunking with LangChain\n",
    "\n",
    "Let's use LangChain's document loaders and text splitters.\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [Document Loaders Conceptual Guide](https://python.langchain.com/docs/concepts/document_loaders/)\n",
    "- [TextLoader Reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html)\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/)\n",
    "- [Text Splitters Conceptual Guide](https://python.langchain.com/docs/concepts/text_splitters/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Total characters: 16,206\n",
      "\n",
      "Document metadata: {'source': 'data/HealthWellnessGuide.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the document using LangChain's TextLoader\n",
    "loader = TextLoader(\"data/HealthWellnessGuide.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Total characters: {sum(len(doc.page_content) for doc in documents):,}\")\n",
    "print(f\"\\nDocument metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 45 chunks\n",
      "\n",
      "Sample chunk (first 300 chars):\n",
      "--------------------------------------------------\n",
      "The Personal Wellness Guide\n",
      "A Comprehensive Resource for Health and Well-being\n",
      "\n",
      "PART 1: EXERCISE AND MOVEMENT\n",
      "\n",
      "Chapter 1: Understanding Exercise Basics\n",
      "\n",
      "Exercise is one of the most important things you can do for your health. Regular physical activity can improve your brain health, help manage weigh...\n"
     ]
    }
   ],
   "source": [
    "# Split documents using RecursiveCharacterTextSplitter\n",
    "# This is more sophisticated than simple character splitting!\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    # Default separators: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    # Tries to keep paragraphs, then sentences, then words together\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(f\"\\nSample chunk (first 300 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Task 6: Setting up Qdrant with Local Embeddings\n",
    "\n",
    "Now we'll use **OllamaEmbeddings** with the `embeddinggemma` model - completely local!\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [OllamaEmbeddings Reference](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)\n",
    "- [Qdrant Vector Store Integration](https://python.langchain.com/docs/integrations/vectorstores/qdrant/)\n",
    "- [Embedding Models Conceptual Guide](https://python.langchain.com/docs/concepts/embedding_models/)\n",
    "- [EmbeddingGemma Overview (Google)](https://ai.google.dev/gemma/docs/embeddinggemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23072ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Created 25 chunks\n",
      "Embedding dimension: 1536\n",
      "Created collection: wellness_knowledge_base_local\n",
      "Adding documents to vector store...\n",
      "Added 25 chunks to vector store\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load the document (no loaders needed)\n",
    "# -----------------------------\n",
    "text = Path(\"data/HealthWellnessGuide.txt\").read_text(encoding=\"utf-8\")\n",
    "documents = [Document(page_content=text, metadata={\"source\": \"HealthWellnessGuide.txt\"})]\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Split into chunks\n",
    "# -----------------------------\n",
    "splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=150, separator=\"\\n\")\n",
    "# or: splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Embeddings (keep as object)\n",
    "# -----------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Qdrant in-memory collection\n",
    "# -----------------------------\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"wellness_knowledge_base_local\"\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)\n",
    ")\n",
    "print(f\"Created collection: {collection_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Vector store + add chunks\n",
    "# -----------------------------\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Adding documents to vector store...\")\n",
    "vector_store.add_documents(chunks)\n",
    "print(f\"Added {len(chunks)} chunks to vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e215bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "- Stage 3: Deep sleep, difficult to wake, body repairs and regenerates\n",
      "- REM Sleep: Brain is active, dreams occur, important for memory and learning\n",
      "Chapter 8: Improving Sleep Quality\n",
      "Sleep hygiene re...\n",
      "\n",
      "--- Document 2 ---\n",
      "Creating an optimal sleep environment:\n",
      "- Temperature: 65-68 degrees Fahrenheit (18-20 Celsius)\n",
      "- Darkness: Use blackout curtains or a sleep mask\n",
      "- Quiet: Consider white noise machines or earplugs\n",
      "- Co...\n",
      "\n",
      "--- Document 3 ---\n",
      "Sleep Checklist:\n",
      "- Room temperature 65-68F\n",
      "- Blackout curtains or sleep mask\n",
      "- No screens 1 hour before bed\n",
      "- No caffeine after 2 PM\n",
      "- Consistent sleep and wake times\n",
      "- Relaxing bedtime routine\n",
      "- Comf...\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "test_results = retriever.invoke(\"How can I improve my sleep?\")\n",
    "\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(test_results, 1):\n",
    "    print(f\"\\n--- Document {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## Task 7: Creating a RAG Tool\n",
    "\n",
    "Now let's wrap our retriever as a tool that the agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG tool created: search_wellness_knowledge\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_wellness_knowledge(query: str) -> str:\n",
    "    \"\"\"Search the wellness knowledge base for information about health, fitness, nutrition, sleep, and mental wellness.\n",
    "    \n",
    "    Use this tool when the user asks questions about:\n",
    "    - Physical health and fitness\n",
    "    - Nutrition and diet\n",
    "    - Sleep and rest\n",
    "    - Mental health and stress management\n",
    "    - General wellness tips\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant wellness information\n",
    "    \"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant information found in the wellness knowledge base.\"\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        formatted_results.append(f\"[Source {i}]:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "print(f\"RAG tool created: {search_wellness_knowledge.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Task 8: Building Agentic RAG from Scratch\n",
    "\n",
    "Now let's put it all together - a complete agentic RAG system built from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "800b7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: search_wellness_knowledge\n",
      "Description: Search the wellness knowledge base for information about health, fitness, nutrition, sleep, and ment...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_wellness_knowledge(query: str) -> str:\n",
    "    \"\"\"Search the wellness knowledge base for information about health, fitness, nutrition, sleep, and mental wellness.\n",
    "    \n",
    "    Use this tool when the user asks questions about:\n",
    "    - Physical health and fitness\n",
    "    - Nutrition and diet\n",
    "    - Sleep and rest\n",
    "    - Mental health and stress management\n",
    "    - General wellness tips\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant wellness information\n",
    "    \"\"\"\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant information found in the wellness knowledge base.\"\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        formatted_results.append(f\"[Source {i}]:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "print(f\"Tool created: {search_wellness_knowledge.name}\")\n",
    "print(f\"Description: {search_wellness_knowledge.description[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fe2a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools created:\n",
      "  - calculate: Evaluate a mathematical expression. Use this for any math ca...\n",
      "  - get_current_time: Get the current date and time. Use this when the user asks a...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression. Use this for any math calculations.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression to evaluate (e.g., '2 + 2', '10 * 5')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Using eval with restricted globals for safety\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when the user asks about the current time or date.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return f\"The current date and time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Create our tool belt\n",
    "tools = [calculate, get_current_time]\n",
    "\n",
    "print(\"Tools created:\")\n",
    "for t in tools:\n",
    "    print(f\"  - {t.name}: {t.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cell-43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools for RAG agent:\n",
      "  - search_wellness_knowledge\n",
      "  - calculate\n",
      "  - get_current_time\n"
     ]
    }
   ],
   "source": [
    "# Define all tools for our RAG agent\n",
    "rag_tools = [search_wellness_knowledge, calculate, get_current_time]\n",
    "\n",
    "# Bind tools to the LLM\n",
    "rag_llm_with_tools = llm.bind_tools(rag_tools)\n",
    "\n",
    "print(\"Tools for RAG agent:\")\n",
    "for t in rag_tools:\n",
    "    print(f\"  - {t.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell-44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent node defined\n"
     ]
    }
   ],
   "source": [
    "# Define the RAG agent components\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a helpful wellness assistant with access to a comprehensive health and wellness knowledge base.\n",
    "\n",
    "Your role is to:\n",
    "1. Answer questions about health, fitness, nutrition, sleep, and mental wellness\n",
    "2. ALWAYS search the knowledge base when the user asks wellness-related questions\n",
    "3. Provide accurate, helpful information based on the retrieved context\n",
    "4. Be supportive and encouraging in your responses\n",
    "5. If you cannot find relevant information, say so honestly\n",
    "\n",
    "Remember: Always cite information from the knowledge base when applicable.\"\"\"\n",
    "\n",
    "def rag_agent_node(state: AgentState):\n",
    "    \"\"\"The RAG agent node - calls the LLM with wellness system prompt.\"\"\"\n",
    "    messages = [SystemMessage(content=RAG_SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "    response = rag_llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Create tool node for RAG tools\n",
    "rag_tool_node = ToolNode(rag_tools)\n",
    "\n",
    "print(\"RAG agent node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33d4f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AgentState messages type: typing.List[langchain_core.messages.base.BaseMessage]\n",
      "should_continue state annotation: <class '__main__.AgentState'>\n",
      "should_continue AgentState messages type: typing.Annotated[list[langchain_core.messages.base.BaseMessage], <function _add_messages_wrapper.<locals>._add_messages at 0x000001D703F40680>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Current AgentState messages type:\", AgentState.__annotations__[\"messages\"])\n",
    "print(\"should_continue state annotation:\", should_continue.__annotations__.get(\"state\"))\n",
    "if \"state\" in should_continue.__annotations__:\n",
    "    old_state = should_continue.__annotations__[\"state\"]\n",
    "    print(\"should_continue AgentState messages type:\", old_state.__annotations__.get(\"messages\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8df15adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "def should_continue(state) -> Literal[\"tools\", \"end\"]:\n",
    "    last = state[\"messages\"][-1]\n",
    "    return \"tools\" if getattr(last, \"tool_calls\", None) else \"end\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cell-45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG built from scratch!\n"
     ]
    }
   ],
   "source": [
    "# Build the RAG agent graph\n",
    "rag_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "rag_workflow.add_node(\"agent\", rag_agent_node)\n",
    "rag_workflow.add_node(\"tools\", rag_tool_node)\n",
    "\n",
    "# Set entry point\n",
    "rag_workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Add conditional edge\n",
    "rag_workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"end\": END}\n",
    ")\n",
    "\n",
    "# Add edge from tools back to agent\n",
    "rag_workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "rag_agent = rag_workflow.compile()\n",
    "\n",
    "print(\"Agentic RAG built from scratch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cell-46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the RAG agent\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(rag_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph image: {e}\")\n",
    "    print(\"\\nGraph structure:\")\n",
    "    print(rag_agent.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cell-47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Agentic RAG (with local models):\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "==================================================\n",
      "Here are evidence-based tips to improve your sleep:\n",
      "\n",
      "- Keep a consistent sleep schedule, even on weekends. Aim for the same bedtime and wake time daily [Source 2] [Source 3].\n",
      "- Create a relaxing 30‚Äì60 minute wind-down routine (e.g., light reading, gentle stretching, or a warm bath) before bed [Source 2].\n",
      "- Optimize your sleep environment: keep your bedroom cool (65‚Äì68¬∞F / 18‚Äì20¬∞C), dark (use blackout curtains or a sleep mask), and quiet (white noise or earplugs can help). Make sure your mattress and pillows are comfortable [Source 1] [Source 2] [Source 3].\n",
      "- Limit screens at least 1 hour before bed; 1‚Äì2 hours is even better [Source 2] [Source 3].\n",
      "- Watch your stimulants and meals: avoid caffeine after 2 PM; limit alcohol and heavy meals close to bedtime [Source 2] [Source 3].\n",
      "- Exercise regularly, but try not to work out too close to bedtime [Source 2].\n",
      "- Aim for 7‚Äì9 hours of sleep per night [Source 3].\n",
      "\n",
      "If sleep problems persist:\n",
      "- Insomnia involves trouble falling asleep, staying asleep, or waking too early; chronic insomnia occurs at least 3 nights per week for 3+ months [Source 1].\n",
      "- Helpful options include Cognitive Behavioral Therapy for Insomnia (CBT-I) and relaxation techniques like progressive muscle relaxation. Some people also find herbal teas such as chamomile or valerian root helpful [Source 1].\n",
      "\n",
      "Would you like help tailoring a plan (e.g., trouble falling asleep vs. waking at night), your current bedtime, and your evening habits?\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG agent\n",
    "print(\"Testing Agentic RAG (with local models):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What are some tips for better sleep?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cell-48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with complex query:\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "==================================================\n",
      "I‚Äôm sorry you‚Äôre dealing with this‚Äîstress and poor sleep can feed into each other. Here‚Äôs a simple, evidence-based plan you can try.\n",
      "\n",
      "For better sleep (start tonight):\n",
      "- Keep a consistent sleep/wake time daily and add a 20‚Äì30 minute wind‚Äëdown (reading, gentle stretching, or a warm bath). (Knowledge base: Chapter 8 ‚Äì Improving Sleep Quality)\n",
      "- Make your bedroom cool, dark, and quiet; aim for 65‚Äì68¬∞F, use blackout curtains/sleep mask, and consider white noise. (KB: Sleep Checklist; Chapter 8)\n",
      "- Limit screens for 1‚Äì2 hours before bed; avoid caffeine after 2 PM; limit alcohol and heavy meals close to bedtime; exercise regularly but not too late. (KB: Chapter 8; Sleep Checklist)\n",
      "- Adults generally need 7‚Äì9 hours per night. If you‚Äôre regularly short on sleep, gradually move your bedtime earlier by 15‚Äì20 minutes every few nights. (KB: Chapter 7 ‚Äì The Science of Sleep)\n",
      "\n",
      "To lower stress:\n",
      "- Quick relief you can use anytime:\n",
      "  - Box breathing: inhale 4, hold 4, exhale 4 (repeat 1‚Äì3 minutes). (KB: Chapter 11 ‚Äì Stress Reduction Techniques)\n",
      "  - Progressive muscle relaxation: tense then release muscle groups from feet to head. (KB: Chapter 11)\n",
      "  - 5‚Äë4‚Äë3‚Äë2‚Äë1 grounding, a short walk (ideally in nature), or calming music. (KB: Chapter 11)\n",
      "- Long-term habits:\n",
      "  - Regular physical activity, supportive social connection, time management and boundaries, and hobbies you enjoy. (KB: Chapter 11)\n",
      "  - Mindfulness meditation: sit comfortably, focus on your breath, and gently return attention when it wanders; start with 5 minutes and build up. (KB: Chapter 12 ‚Äì Mindfulness and Meditation)\n",
      "\n",
      "When to get extra help:\n",
      "- If insomnia lasts at least 3 nights/week for 3+ months, consider Cognitive Behavioral Therapy for Insomnia (CBT‚ÄëI)‚Äîa first‚Äëline, effective treatment. (KB: Chapter 9 ‚Äì Insomnia and CBT‚ÄëI)\n",
      "- If stress feels overwhelming or you notice significant anxiety or low mood, reach out to a healthcare professional. (KB: Chapter 10‚Äì11)\n",
      "\n",
      "Your math: 6 hours/night √ó 7 nights = 42 hours total for the week.\n",
      "\n",
      "Would you like help tailoring a wind‚Äëdown routine and caffeine/screen plan to your schedule?\n"
     ]
    }
   ],
   "source": [
    "# Test with a complex query requiring both RAG and calculation\n",
    "print(\"Testing with complex query:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(\n",
    "        content=\"I'm stressed and sleeping poorly. What should I do? Also, if I sleep 6 hours a night for a week, how many total hours is that?\"\n",
    "    )]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cell-49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent decision-making (should NOT use RAG):\n",
      "==================================================\n",
      "\n",
      "Final Response:\n",
      "125 * 8 = 1000\n"
     ]
    }
   ],
   "source": [
    "# Test that the agent knows when NOT to use RAG\n",
    "print(\"Testing agent decision-making (should NOT use RAG):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is 125 * 8?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ùì Question #3:\n",
    "\n",
    "Compare the experience of building an agent from scratch with LangGraph versus using `create_agent` from Session 3. What are the trade-offs between control and convenience? When would you choose one approach over the other?\n",
    "\n",
    "##### Answer:\n",
    "*Building agent from scratch **versus** 'create_agent' had the following pros and cons:\n",
    "Pros:\n",
    "1. Full control over the loop\n",
    "2. Add as much logging and custom logging as needed.\n",
    "\n",
    "Cons:\n",
    "1. More coding involved and prone to making mistakes.\n",
    "2. More time to build and test.\n",
    "\n",
    "When I would choose which approach:\n",
    "If I am building a security related tool where security is the major concern I would build the agent from scratch. If not if I am testing a quick POC, I think '*create_agent*' is besyt suited\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## ‚ùì Question #4:\n",
    "\n",
    "We used local models (gpt-oss:20b and embeddinggemma) instead of cloud APIs. What are the advantages and disadvantages of this approach? \n",
    "\n",
    "##### Answer:\n",
    "*Advantages of using local models (gpt-oss:20b and embeddinggemma) instead of cloud APIs\n",
    "\n",
    "1. Data Control and Privacy\n",
    "2. Chepaer than Cloud APIs\n",
    "3. Works offline\n",
    "\n",
    "Best suited for a building a tool with sensitive/legal documents where external API calls can be very risky.\n",
    "\n",
    "Disadvantages:\n",
    "1. Latency issues: They are slower\n",
    "2. Too buch burden on the local system resources since it runs on your computer.\n",
    "3. Model Quality gap: Won't perform as good as the cloud models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "---\n",
    "## üèóÔ∏è Activity #2: Extend the Agent with Memory\n",
    "\n",
    "LangGraph supports **checkpointing** which enables conversation memory across invocations.\n",
    "\n",
    "Your task: Add memory to the RAG agent so it can:\n",
    "1. Remember previous questions in the conversation\n",
    "2. Reference past context when answering new questions\n",
    "3. Build on previous answers\n",
    "\n",
    "Hint: Use `MemorySaver` from `langgraph.checkpoint.memory` and pass a `thread_id` in the config.\n",
    "\n",
    "**üìö Documentation:**\n",
    "- [LangGraph Persistence & Memory](https://langchain-ai.github.io/langgraph/concepts/persistence/)\n",
    "- [How to add memory to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "- [MemorySaver Reference](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cell-55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph compiled with MemorySaver (conversation memory enabled)\n",
      "\n",
      "--- Turn 1 ---\n",
      "Progressive overload is the practice of adding a small load (2‚Äì5%) or 1 rep weekly while keeping 1‚Äì3 reps in reserve (RIR). The load is increased after topping the rep range. This method helps in gradually increasing the intensity of workouts to promote strength and muscle gains. (Source: KB2)\n",
      "\n",
      "--- Turn 2 (references past context) ---\n",
      "Sure! Here's a simple weekly progression example based on progressive overload:\n",
      "\n",
      "Week 1: Lift a weight for 8 reps with 2 reps in reserve (RIR).\n",
      "Week 2: Add a small load increase of about 2-5% or try to do 9 reps with the same weight, keeping 1-3 reps in reserve.\n",
      "Week 3: Add another small load increase of 2-5% or aim for 10 reps, still keeping 1-3 reps in reserve.\n",
      "Week 4: Once you reach the top of your rep range (e.g., 10 reps), increase the load again by 2-5% and drop back to 8 reps with 1-3 reps in reserve.\n",
      "\n",
      "This gradual increase in load or reps while maintaining some reps in reserve helps ensure steady progress without overtraining.\n",
      "\n",
      "--- Turn 3 (builds on previous answers) ---\n",
      "If you stall for about 2 weeks, it is recommended to reduce your training volume by approximately 30‚Äì50% for 1 week, which is known as a deload. This helps to reduce fatigue and allow your body to recover, potentially overcoming the performance stall (Source: KB1). Additionally, ensuring good sleep and hydration is important for recovery and performance (Source: KB3).\n"
     ]
    }
   ],
   "source": [
    "# ---- Imports (with a few safe fallbacks for different versions) ----\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "try:\n",
    "    from langgraph.graph import START\n",
    "except Exception:\n",
    "    # Older versions: you can also set entry point instead of START\n",
    "    START = \"__start__\"\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# MemorySaver import path can vary slightly by version\n",
    "try:\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "except Exception:\n",
    "    from langgraph.checkpoint import MemorySaver\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Define State (messages accumulate + memory across invocations)\n",
    "# =========================\n",
    "class AgentState(TypedDict):\n",
    "    # Annotated + add_messages tells LangGraph how to append new messages to history\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Define Tools (RAG-like KB search + calculator)\n",
    "# =========================\n",
    "class KBArgs(BaseModel):\n",
    "    query: str = Field(..., description=\"Question to search in the wellness KB.\")\n",
    "    top_k: int = Field(3, description=\"How many snippets to return.\")\n",
    "\n",
    "@tool(\"search_wellness_knowledge\", args_schema=KBArgs)\n",
    "def search_wellness_knowledge(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Search the wellness knowledge base and return evidence snippets.\n",
    "    Use this when you need factual wellness guidance (progressive overload, deload, recovery).\n",
    "    \"\"\"\n",
    "    kb = [\n",
    "        {\"id\": \"KB1\", \"title\": \"Deload Guidance\", \"text\": \"If fatigue stalls performance for ~2+ weeks, reduce volume by ~30‚Äì50% for 1 week (deload).\"},\n",
    "        {\"id\": \"KB2\", \"title\": \"Progressive Overload\", \"text\": \"Add small load (2‚Äì5%) or 1 rep weekly while keeping 1‚Äì3 reps in reserve (RIR). Increase load after topping rep range.\"},\n",
    "        {\"id\": \"KB3\", \"title\": \"Recovery Basics\", \"text\": \"Sleep and hydration strongly influence recovery, performance, and muscle gain.\"},\n",
    "    ]\n",
    "    q = query.lower()\n",
    "    scored = []\n",
    "    for item in kb:\n",
    "        score = sum(1 for w in q.split() if w in item[\"title\"].lower() or w in item[\"text\"].lower())\n",
    "        scored.append((score, item))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    hits = [item for s, item in scored[:top_k] if s > 0] or [scored[0][1]]\n",
    "    return {\"query\": query, \"results\": hits}\n",
    "\n",
    "class CalcArgs(BaseModel):\n",
    "    expression: str = Field(..., description=\"Math expression like '19*23'.\")\n",
    "\n",
    "@tool(\"calculate\", args_schema=CalcArgs)\n",
    "def calculate(expression: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a simple math expression.\n",
    "    Use this when the user asks for arithmetic.\n",
    "    \"\"\"\n",
    "    allowed = set(\"0123456789+-*/(). %\")\n",
    "    if any(ch not in allowed for ch in expression):\n",
    "        raise ValueError(\"Expression contains unsupported characters.\")\n",
    "    result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "    return {\"expression\": expression, \"result\": result}\n",
    "\n",
    "TOOLS = [search_wellness_knowledge, calculate]\n",
    "TOOL_BY_NAME = {t.name: t for t in TOOLS}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) LLM (tool-calling enabled)\n",
    "# =========================\n",
    "# Use a tool-capable model you have access to. You can swap \"gpt-4.1-mini\" -> \"gpt-5\" if enabled.\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0).bind_tools(TOOLS)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Nodes\n",
    "# =========================\n",
    "def agent_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Call the model with the full conversation history.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def tool_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Execute tools for the most recent AI message that contains tool_calls.\n",
    "    IMPORTANT: If AI emitted tool_calls, the very next messages MUST be ToolMessages\n",
    "    for each tool_call_id (OpenAI tool-calling protocol).\n",
    "    \"\"\"\n",
    "    # Find latest message with tool_calls (usually the last AI message)\n",
    "    tool_calls = []\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        tc = getattr(msg, \"tool_calls\", None) or []\n",
    "        if tc:\n",
    "            tool_calls = tc\n",
    "            break\n",
    "\n",
    "    if not tool_calls:\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    tool_messages: List[ToolMessage] = []\n",
    "\n",
    "    for tc in tool_calls:\n",
    "        tool_name = tc.get(\"name\")\n",
    "        tool_args = tc.get(\"args\", {}) or {}\n",
    "        tool_id = tc.get(\"id\")\n",
    "\n",
    "        try:\n",
    "            if tool_name not in TOOL_BY_NAME:\n",
    "                raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "            output = TOOL_BY_NAME[tool_name].invoke(tool_args)\n",
    "            tool_messages.append(ToolMessage(content=str(output), tool_call_id=tool_id))\n",
    "        except Exception as e:\n",
    "            tool_messages.append(ToolMessage(content=f\"Tool '{tool_name}' error: {type(e).__name__}: {e}\", tool_call_id=tool_id))\n",
    "\n",
    "    return {\"messages\": tool_messages}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Routing logic (continue if tool_calls exist)\n",
    "# (No AgentState type annotation here to avoid notebook schema mismatch issues.)\n",
    "# =========================\n",
    "def should_continue(state):\n",
    "    last = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last, \"tool_calls\", None) or []\n",
    "    return \"tools\" if tool_calls else \"end\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Build + compile graph with MemorySaver (checkpointing)\n",
    "# =========================\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Entry point\n",
    "try:\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "except Exception:\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Conditional loop\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "rag_agent = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"‚úÖ Graph compiled with MemorySaver (conversation memory enabled)\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Helper: run a turn with a thread_id (same thread_id == same memory)\n",
    "# =========================\n",
    "def chat(thread_id: str, user_text: str) -> str:\n",
    "    out = rag_agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_text)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "    )\n",
    "    # last AI message content\n",
    "    for msg in reversed(out[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage) and msg.content:\n",
    "            return msg.content\n",
    "    return \"(No assistant message found.)\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Demo: memory across invocations\n",
    "# =========================\n",
    "THREAD = \"wellness_demo_thread_1\"\n",
    "\n",
    "print(\"\\n--- Turn 1 ---\")\n",
    "print(chat(THREAD, \"What is progressive overload? Use the wellness KB and cite your source ids.\"))\n",
    "\n",
    "print(\"\\n--- Turn 2 (references past context) ---\")\n",
    "print(chat(THREAD, \"Based on that, give me a simple weekly progression example.\"))\n",
    "\n",
    "print(\"\\n--- Turn 3 (builds on previous answers) ---\")\n",
    "print(chat(THREAD, \"If I stall for 2 weeks, what should I change? Again cite KB sources.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this session, we:\n",
    "\n",
    "1. **Built agents from scratch** using LangGraph's low-level primitives (StateGraph, nodes, edges)\n",
    "2. **Used local open-source models** with Ollama (gpt-oss:20b + embeddinggemma)\n",
    "3. **Transitioned to LangChain** for document loading and text splitting\n",
    "4. **Created an Agentic RAG system** that intelligently decides when to retrieve information\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **StateGraph** gives you full control over agent architecture\n",
    "- **Conditional edges** enable dynamic routing based on LLM decisions\n",
    "- **Local models** provide privacy and cost savings, with trade-offs in performance\n",
    "- **LangSmith** provides crucial visibility regardless of where your models run\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that you understand the fundamentals, you can:\n",
    "- Add more sophisticated routing logic\n",
    "- Implement human-in-the-loop patterns\n",
    "- Build multi-agent systems\n",
    "- Deploy to production with LangGraph Platform\n",
    "\n",
    "**üìö Further Reading:**\n",
    "- [LangGraph How-To Guides](https://langchain-ai.github.io/langgraph/how-tos/)\n",
    "- [Human-in-the-Loop Patterns](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/)\n",
    "- [Multi-Agent Architectures](https://langchain-ai.github.io/langgraph/concepts/multi_agent/)\n",
    "- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
